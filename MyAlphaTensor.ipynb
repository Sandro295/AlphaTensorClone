{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gDzvLsjANl2t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "\n",
        "from typing import Tuple, List, Callable, Dict\n",
        "\n",
        "BASE_CHECKPOINT_DIR = \"checkpoints\"\n",
        "BASE_CHECKPOINT_DATA_DIR = \"games\"\n",
        "SAVE_DIR_SYNT = \"./.data_alpha_tensor/synthetic_data\"\n",
        "SAVE_COB_DIR = \"./.data_alpha_tensor/cob_matrices\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural net\n",
        "## Torso a.k.a. the transformer\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Fr04K85XNw2V"
      },
      "outputs": [],
      "source": [
        "class PositionEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[: x.size(0)]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QjqnxRQJOKzk"
      },
      "outputs": [],
      "source": [
        "class AttentionDenseBlock(torch.nn.Module):\n",
        "    def __init__(self, inner_size: int, multiplier: int = 4):\n",
        "        super().__init__()\n",
        "        self.norm = torch.nn.LayerNorm(inner_size)\n",
        "        self.linear = torch.nn.Linear(inner_size, inner_size * multiplier)\n",
        "        self.activation = torch.nn.GELU()\n",
        "        self.linear_final = torch.nn.Linear(\n",
        "            inner_size * multiplier, inner_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x_temp = self.activation(self.linear(self.norm(x)))\n",
        "        return x + self.linear_final(x_temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kwJ0RIo9OCVG"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(torch.nn.Module):\n",
        "    def __init__(self, x_size: int, y_size: int, proj_dim: int):\n",
        "        # x_size = N_x\n",
        "        # y_size = N_y\n",
        "        super(AttentionHead, self).__init__()\n",
        "        self.proj_dim_isqrt = 1 / torch.sqrt(torch.tensor(proj_dim))\n",
        "        self.queries_proj_layer = torch.nn.Linear(x_size, proj_dim)\n",
        "        self.keys_proj_layer = torch.nn.Linear(y_size, proj_dim)\n",
        "        self.values_proj_layer = torch.nn.Linear(y_size, proj_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: torch.Tensor, mask: bool = False):\n",
        "        queries = self.queries_proj_layer(x)\n",
        "        keys = self.keys_proj_layer(y)\n",
        "        values = self.values_proj_layer(y)\n",
        "        attention = F.softmax(\n",
        "            torch.matmul(queries, keys.transpose(-2, -1))\n",
        "            * self.proj_dim_isqrt,\n",
        "            dim=-1,\n",
        "        )\n",
        "        if mask:\n",
        "            attention = torch.triu(attention, diagonal=1)\n",
        "        output = torch.matmul(attention, values)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bE9GsRfwN-oH"
      },
      "outputs": [],
      "source": [
        "class AlphaMultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        x_dim: int,\n",
        "        y_dim: int,\n",
        "        proj_dim: int = 32,\n",
        "        n_heads: int = 16,\n",
        "        multiplier: int = 4,\n",
        "    ):\n",
        "        # x_dim = size of the last dimension of x\n",
        "        # y_dim = size of the last dimension of y\n",
        "        super().__init__()\n",
        "        self.norm_layer_x = torch.nn.LayerNorm(x_dim)\n",
        "        self.norm_layer_y = torch.nn.LayerNorm(y_dim)\n",
        "        self.module_list = torch.nn.ModuleList([AttentionHead(x_dim, y_dim, proj_dim) for _ in range(n_heads)])\n",
        "        self.linear = torch.nn.Linear(n_heads * proj_dim, x_dim)\n",
        "\n",
        "        self.dense = AttentionDenseBlock(x_dim, multiplier)\n",
        "\n",
        "    def forward(self, x: torch.nn.Module, y: torch.nn.Module, mask: bool = False):\n",
        "        # x.size = (Nx, c1), y.size = (Ny, c2)\n",
        "        x_norm = self.norm_layer_x(x)\n",
        "        y_norm = self.norm_layer_y(y)\n",
        "        temp = torch.cat([layer(x_norm, y_norm, mask) for layer in self.module_list], dim=-1)\n",
        "        x = x + self.linear(temp)\n",
        "        return self.dense(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "NPYngms8N7Gi"
      },
      "outputs": [],
      "source": [
        "class PolicyHeadDoubleAttention(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_steps: int,\n",
        "        n_heads: int,\n",
        "        n_feat: int,\n",
        "        emb_size: int,\n",
        "        emb_dim: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        d_model = n_feat * n_heads\n",
        "        self.layer_norm1 = torch.nn.LayerNorm(d_model)\n",
        "        self.attention1 = AlphaMultiHeadAttention(d_model, d_model)\n",
        "        self.drop1 = torch.nn.Dropout()\n",
        "        self.layer_norm2 = torch.nn.LayerNorm(d_model)\n",
        "        self.attention2 = AlphaMultiHeadAttention(d_model, emb_dim)\n",
        "        self.drop2 = torch.nn.Dropout()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, e: torch.Tensor):\n",
        "        x = self.layer_norm1(x)\n",
        "        c = self.attention1(x, x, mask=True)\n",
        "        c = self.drop1(c)\n",
        "        x = x + c\n",
        "        x = self.layer_norm2(x)\n",
        "        c = self.attention2(x, e, mask=False)\n",
        "        c = self.drop2(c)\n",
        "        x = x + c\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FK78AUqqNtgB"
      },
      "outputs": [],
      "source": [
        "class PolicyHeadCore(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        emb_size: int,\n",
        "        emb_dim: int,\n",
        "        n_steps: int,\n",
        "        n_logits: int,\n",
        "        n_feat: int = 32,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(n_logits, n_feat * n_heads)\n",
        "        self.position_encoding = PositionEncoding(n_feat * n_heads)\n",
        "        self.decoders = torch.nn.ModuleList(\n",
        "            [\n",
        "                PolicyHeadDoubleAttention(\n",
        "                    n_steps, n_heads, n_feat, emb_size, emb_dim\n",
        "                )\n",
        "                for _ in range(n_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(n_feat * n_heads, n_logits)\n",
        "\n",
        "    def forward(self, a: torch.Tensor, e: torch.Tensor):\n",
        "        x = self.position_encoding(self.embedding(a))\n",
        "        for layer in self.decoders:\n",
        "            x = layer(x, e)\n",
        "        o = self.linear2(self.relu(x))\n",
        "        return o, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "u8VPIW6tB5fC"
      },
      "outputs": [],
      "source": [
        "def sample_from_logits(a):\n",
        "    # returns a sampled element and the associated probability\n",
        "    # since cross entropy is run during training we expect logits\n",
        "    # to be probabilities yet.\n",
        "    probs = torch.cumsum(F.softmax(a, dim=-1), dim=-1)\n",
        "    random_vals = torch.rand(probs.shape[0]).unsqueeze(-1).to(a.device)\n",
        "    n_classes = a.shape[-1]\n",
        "    new_a_idx = torch.argmax(1.0 * (probs > random_vals), dim=-1)\n",
        "    index_bias = torch.arange(0, len(new_a_idx)).to(a.device) * n_classes\n",
        "    probs = torch.take(probs, new_a_idx + index_bias)\n",
        "    # new_a = F.one_hot(new_a_idx, n_classes)\n",
        "    return new_a_idx, probs\n",
        "\n",
        "class PolicyHead(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        emb_size: int,\n",
        "        emb_dim: int,\n",
        "        n_steps: int,\n",
        "        n_logits: int,\n",
        "        n_samples: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_logits = n_logits\n",
        "        self.n_samples = n_samples\n",
        "        self.n_steps = n_steps\n",
        "        self.core = PolicyHeadCore(emb_size, emb_dim, n_steps, n_logits)\n",
        "\n",
        "    def _train_forward(self, e: torch.Tensor, g: torch.Tensor):\n",
        "        # e is the embedding, shape = (N, m, c)\n",
        "        # g represents the previous actions, when training it represents the\n",
        "        # list of correct actions, thus we need to shift them (since we do not\n",
        "        # want to consider also the latest, correct action when predicting).\n",
        "        # g has shape (N, N_steps) and it is a one-hot encoding of N_logits\n",
        "        g = torch.roll(g, shifts=-1, dims=1)\n",
        "        # the first raw will have attention zero during training\n",
        "        # g = F.one_hot(g, self.n_logits).float()\n",
        "        o, z = self.core(g, e)\n",
        "        return o, z[:, 0]\n",
        "\n",
        "    def _eval_forward(self, e: torch.Tensor):\n",
        "        bs = e.shape[0]\n",
        "        future_g = (\n",
        "            torch.zeros((bs, self.n_samples, self.n_steps)).long().to(e.device)\n",
        "        )\n",
        "        ps = torch.ones((bs, self.n_samples)).to(e.device)\n",
        "        e = e.unsqueeze(1).repeat(1, self.n_samples, 1, 1)\n",
        "\n",
        "        future_g = future_g.view(-1, self.n_steps)\n",
        "        ps = ps.view(-1)\n",
        "        e = e.view(-1, e.shape[-2], e.shape[-1])\n",
        "        for i in range(self.n_steps):\n",
        "            o_s, z_s = self.core(future_g[:, : i + 1], e)\n",
        "            future_g[:, i], p_i = sample_from_logits(o_s[:, i])\n",
        "            ps *= p_i\n",
        "        future_g = future_g.view(bs, self.n_samples, self.n_steps)\n",
        "        ps = ps.view(bs, self.n_samples)\n",
        "        return (\n",
        "            future_g,\n",
        "            ps,\n",
        "            z_s[:, 0].view(bs, self.n_samples, *z_s.shape[2:]).mean(1),\n",
        "        )\n",
        "\n",
        "    def forward(self, e: torch.Tensor, g: torch.Tensor = None):\n",
        "        if g is None:\n",
        "            return self._eval_forward(e)\n",
        "        return self._train_forward(e, g)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZZk1PfZ3wD7"
      },
      "source": [
        "## Value\n",
        "Value head is a multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2HDNS1dyYgH_"
      },
      "outputs": [],
      "source": [
        "class ValueHeadCore(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.relu(self.linear(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uiLSqKmu3vfY"
      },
      "outputs": [],
      "source": [
        "class ValueHead(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size: int, hidden_size: int = 512, output_size: int = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            *(\n",
        "                [ValueHeadCore(input_size, hidden_size)]\n",
        "                + [ValueHeadCore(hidden_size, hidden_size)] * 2\n",
        "            )\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.linear(self.layers(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "EbaZ0jog4als"
      },
      "outputs": [],
      "source": [
        "class AttentionDenseBlock(torch.nn.Module):\n",
        "    def __init__(self, inner_size: int, multiplier: int = 4):\n",
        "        super().__init__()\n",
        "        self.norm = torch.nn.LayerNorm(inner_size)\n",
        "        self.linear = torch.nn.Linear(inner_size, inner_size * multiplier)\n",
        "        self.activation = torch.nn.GELU()\n",
        "        self.linear_final = torch.nn.Linear(\n",
        "            inner_size * multiplier, inner_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x_temp = self.activation(self.linear(self.norm(x)))\n",
        "        return x + self.linear_final(x_temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sPrcYLmI4dre"
      },
      "outputs": [],
      "source": [
        "class TorsoAttentiveModes(torch.nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        # input_dim = c\n",
        "        super().__init__()\n",
        "        self.attention = AlphaMultiHeadAttention(\n",
        "            input_dim,\n",
        "            input_dim,\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        # x1.size = x2.size = x3.size = (N, S, S, c)\n",
        "        # where N is the batch size\n",
        "        size = x1.shape[-2]\n",
        "        input_list = [x1, x2, x3]\n",
        "        for m1, m2 in [(0, 1), (2, 0), (1, 2)]:\n",
        "            matrix = torch.cat([input_list[m1], input_list[m2]], dim=-2)\n",
        "            # matrix_size = (N, S, 2S, c)\n",
        "            out = self.attention(matrix, matrix)\n",
        "            input_list[m1] = out[:, :, :size]\n",
        "            input_list[m2] = out[:, :, size:]\n",
        "        return input_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "nEEG5_Mo4eyu"
      },
      "outputs": [],
      "source": [
        "class TorsoModel(torch.nn.Module):\n",
        "    \"\"\"Torso model of OpenAlphaTensor.\n",
        "\n",
        "    It maps an input tensor of shape (N, T, S, S, S) to (N, 3S*S, c), where:\n",
        "\n",
        "        N is the batch size;\n",
        "        T is the context size (size of the history + 1);\n",
        "        S is the number of elements in each matrix to be multiplied;\n",
        "        c is the output dimensionality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scalars_size: int,\n",
        "        input_size: int,\n",
        "        tensor_length: int,\n",
        "        out_size: int,\n",
        "    ):\n",
        "        # scalar_size = s\n",
        "        # input_size = S\n",
        "        # tensor_length = T\n",
        "        # out_size = c\n",
        "        super(TorsoModel, self).__init__()\n",
        "        self.linears_1 = torch.nn.ModuleList(\n",
        "            [\n",
        "                torch.nn.Linear(scalars_size, input_size * input_size)\n",
        "                for _ in range(3)\n",
        "            ]\n",
        "        )\n",
        "        self.linears_2 = torch.nn.ModuleList(\n",
        "            [\n",
        "                torch.nn.Linear(input_size * tensor_length + 1, out_size)\n",
        "                for _ in range(3)\n",
        "            ]\n",
        "        )\n",
        "        self.attentive_modes = torch.nn.ModuleList(\n",
        "            [TorsoAttentiveModes(out_size) for _ in range(4)] # the paper has 8 attentive modes, but it takes a long time to train\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, scalars: torch.Tensor):\n",
        "        # x.size = (N, T, S, S, S)\n",
        "        # scalars.size = (N, s)\n",
        "        batch_size = x.shape[0]\n",
        "        S = x.shape[-1]\n",
        "        T = x.shape[1]\n",
        "        x1 = x.permute(0, 2, 3, 4, 1).reshape(batch_size, S, S, S * T)\n",
        "        x2 = x.permute(0, 4, 2, 3, 1).reshape(batch_size, S, S, S * T)\n",
        "        x3 = x.permute(0, 3, 4, 2, 1).reshape(batch_size, S, S, S * T)\n",
        "        input_list = [x1, x2, x3]\n",
        "        for i in range(3):\n",
        "            temp = self.linears_1[i](scalars).reshape(batch_size, S, S, 1)\n",
        "            input_list[i] = torch.cat([input_list[i], temp], dim=-1)\n",
        "            input_list[i] = self.linears_2[i](input_list[i])\n",
        "        x1, x2, x3 = input_list\n",
        "        for layer in self.attentive_modes:\n",
        "            x1, x2, x3 = layer(x1, x2, x3)\n",
        "        return torch.stack([x1, x2, x3], dim=2).reshape(\n",
        "            batch_size, 3 * S * S, -1\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "DXwRdT37GN4o"
      },
      "outputs": [],
      "source": [
        "class QuantileLoss(torch.nn.Module):\n",
        "    def __init__(self, delta: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.huber_loss = torch.nn.HuberLoss(reduction=\"none\", delta=delta)\n",
        "\n",
        "    def forward(self, q: torch.Tensor, g: torch.Tensor):\n",
        "        n = q.shape[-1]\n",
        "        tau = torch.arange(0, n).unsqueeze(0).to(q.device) / n\n",
        "        h = self.huber_loss(g, q)\n",
        "        k = torch.abs(tau - (g - q > 0).float())\n",
        "        return torch.mean(h * k)\n",
        "\n",
        "\n",
        "class ValueRiskManagement(torch.nn.Module):\n",
        "    def __init__(self, u_q: float = 0.75):\n",
        "        super(ValueRiskManagement, self).__init__()\n",
        "        self.u_q = u_q\n",
        "\n",
        "    def forward(self, q: torch.Tensor):\n",
        "        # q shape = (N, n)\n",
        "        j = int(self.u_q * q.shape[-1])\n",
        "        return torch.mean(q[:, j:], dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tNa0_KHQEKOI"
      },
      "outputs": [],
      "source": [
        "class AlphaTensorModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tensor_length: int,\n",
        "        input_size: int,\n",
        "        scalars_size: int,\n",
        "        emb_dim: int,\n",
        "        n_steps: int,\n",
        "        n_logits: int,\n",
        "        n_samples: int,\n",
        "    ):\n",
        "        # scalar_size = s\n",
        "        # input_size = S\n",
        "        # tensor_length = T\n",
        "        # emb_dim = c\n",
        "        super().__init__()\n",
        "        self.tensor_length = tensor_length\n",
        "        self.input_size = input_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.torso = TorsoModel(\n",
        "            scalars_size, input_size, tensor_length, emb_dim\n",
        "        )\n",
        "        emb_size = 3 * input_size * input_size\n",
        "        print(\"Build policy head\")\n",
        "        self.policy_head = PolicyHead(\n",
        "            emb_size, emb_dim, n_steps, n_logits, n_samples\n",
        "        )\n",
        "        print(\"Build value head\")\n",
        "        self.value_head = ValueHead(\n",
        "            256 \n",
        "        )  # value dependent on num_head and proj_dim\n",
        "        self.policy_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        self.quantile_loss_fn = QuantileLoss()\n",
        "        self.risk_value_management = ValueRiskManagement()\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def _train_forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        s: torch.Tensor,\n",
        "        g_action: torch.Tensor,\n",
        "        g_value: torch.Tensor,\n",
        "    ):\n",
        "        # shapes\n",
        "        # x = (N, T, S, S, S)\n",
        "        # s = (N, s)\n",
        "        # g_action = (N, N_steps)\n",
        "        # g_value = (N, )\n",
        "        e = self.torso(x, s)\n",
        "        o, z1 = self.policy_head(e, g_action)\n",
        "        l_policy = self.policy_loss_fn(\n",
        "            o.reshape(-1, o.shape[-1]), g_action.reshape(-1)\n",
        "        )\n",
        "        q = self.value_head(z1)\n",
        "        l_value = self.quantile_loss_fn(q, g_value.float())\n",
        "        return l_policy, l_value\n",
        "\n",
        "    def _eval_forward(self, x: torch.Tensor, s: torch.Tensor):\n",
        "        e = self.torso(x, s)\n",
        "        a, p, z1 = self.policy_head(e)\n",
        "        q = self.value_head(z1)\n",
        "        q = self.risk_value_management(q)\n",
        "        return a, p, q\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        s: torch.Tensor,\n",
        "        g_action: torch.Tensor = None,\n",
        "        g_value: torch.Tensor = None,\n",
        "    ):\n",
        "        if g_action is None:\n",
        "            return self._eval_forward(x, s)\n",
        "        else:\n",
        "            assert g_value is not None\n",
        "            return self._train_forward(x, s, g_action, g_value)\n",
        "\n",
        "    @property\n",
        "    def n_logits(self):\n",
        "        return self.policy_head.n_logits\n",
        "\n",
        "    @property\n",
        "    def n_steps(self):\n",
        "        return self.policy_head.n_steps\n",
        "\n",
        "    @property\n",
        "    def n_samples(self):\n",
        "        return self.policy_head.n_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "budB0J8aGUE8"
      },
      "source": [
        "# Some utility funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "mhq_TdnOB_Va"
      },
      "outputs": [],
      "source": [
        "def get_scalars(input_tensor: torch.Tensor, t_step: int, with_bs: bool = True):\n",
        "    \"\"\"Adds the time step to the current state tensor.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (torch.Tensor): Current state tensor.\n",
        "        t_step (int): Current time step.\n",
        "        with_bs (bool, optional): Whether the batch size is present in the\n",
        "        input tensor.\n",
        "    \"\"\"\n",
        "    # scalars containing the iteration time\n",
        "    if with_bs:\n",
        "        bs = input_tensor.shape[0]\n",
        "        scalars = torch.zeros((bs, 1))\n",
        "        scalars[:, 0] = t_step\n",
        "    else:\n",
        "        scalars = torch.tensor(t_step).unsqueeze(-1).float()\n",
        "    return scalars\n",
        "\n",
        "\n",
        "def map_triplet_to_action(\n",
        "    triplet: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "    base: int,\n",
        "    n_steps: int,\n",
        "    add_bias: bool = True,\n",
        "):\n",
        "    \"\"\"Maps a triplet of tensors to an action.\n",
        "\n",
        "    Args:\n",
        "        triplet (Tuple[torch.Tensor, torch.Tensor, torch.Tensor]): Triplet of\n",
        "        tensors u, v, and w.\n",
        "        base (int): Base used for the conversion.\n",
        "        n_steps (int): Number of steps in the action.\n",
        "        add_bias (bool, optional): Whether to add a bias to the action.\n",
        "    \"\"\"\n",
        "    # map the triplet to an action. First, we concatenate the three tensors and\n",
        "    # then we convert it to an action using the given base representation. Each\n",
        "    # element is converted using the formula:\n",
        "    #   action += element * base^(element_index)\n",
        "    u, v, w = triplet\n",
        "    n_dim = u.ndim\n",
        "    action = torch.cat((u, v, w), dim=-1)\n",
        "    action = action.reshape(-1, n_steps, action.shape[-1] // n_steps)\n",
        "    if n_dim == 1:\n",
        "        action = action.squeeze(0)\n",
        "    if add_bias:\n",
        "        action = action + base // 2\n",
        "    action = action * torch.tensor(\n",
        "        [base**i for i in range(action.shape[-1])]\n",
        "    )\n",
        "    action = action.sum(dim=-1)\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "lXAH1Jw_B44S"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.script\n",
        "def _single_action_to_triplet(\n",
        "    action_val: int,\n",
        "    basis: int,\n",
        "    out_dim: int,\n",
        "    bias: int,\n",
        "    device: str,\n",
        "):\n",
        "    \"\"\"Converts an action to the original triplet (u, v, w) that generated it.\n",
        "\n",
        "    Args:\n",
        "        action_val (int): Action to convert.\n",
        "        basis (int): Basis used for the conversion.\n",
        "        out_dim (int): Output dimension.\n",
        "        bias (int): Bias to subtract from the action.\n",
        "        device (str): Name of the torch device to use.\n",
        "    \"\"\"\n",
        "    triplet = torch.zeros(out_dim).to(device)\n",
        "    if action_val > 0:\n",
        "        idx = int(\n",
        "            torch.log(torch.tensor(action_val))\n",
        "            // torch.log(torch.tensor(basis))\n",
        "        )\n",
        "    else:\n",
        "        idx = 0\n",
        "    while idx >= 0:\n",
        "        temp = int(basis**idx)\n",
        "        triplet[idx] = action_val // temp - bias\n",
        "        action_val = action_val - temp\n",
        "        idx -= 1\n",
        "    return triplet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cnfexjQSBwYx"
      },
      "outputs": [],
      "source": [
        "def map_action_to_triplet(\n",
        "    action_tensor: torch.Tensor,\n",
        "    cardinality: int = 5,\n",
        "    vector_size: int = 5,\n",
        "    add_bias: bool = True,\n",
        "):\n",
        "    \"\"\"Maps a batch of actions to the batch of triplets that generated them.\n",
        "\n",
        "    Args:\n",
        "        action_tensor (torch.Tensor): Batch of actions.\n",
        "        cardinality (int, optional): Cardinality of the action space.\n",
        "        vector_size (int, optional): Size of the vector.\n",
        "        add_bias (bool, optional): Whether to use bias.\n",
        "    \"\"\"\n",
        "    # map the action to a triplet. The action is converted to a base 5\n",
        "    # representation and then the three elements are extracted from it.\n",
        "    # The action has shape (bs, n_steps) and it contains the token for\n",
        "    # recreating u, v and w. The token is a number between 0 and n_logits.\n",
        "    action_shape = action_tensor.shape\n",
        "    action_tensor = action_tensor.reshape(-1)\n",
        "    if add_bias:\n",
        "        bias = cardinality // 2\n",
        "    else:\n",
        "        bias = 0\n",
        "    triplets = torch.stack(\n",
        "        [\n",
        "            _single_action_to_triplet(\n",
        "                action_tensor[idx],\n",
        "                cardinality,\n",
        "                vector_size,\n",
        "                bias,\n",
        "                action_tensor.device,\n",
        "            )\n",
        "            for idx in range(len(action_tensor))\n",
        "        ]\n",
        "    )\n",
        "    final_size = triplets.shape[-1]\n",
        "    return triplets.reshape((*action_shape, final_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5lL_2mxRKuuU"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data(\n",
        "    tensor_size: int,\n",
        "    n_data: int,\n",
        "    limit_rank: int,\n",
        "    prob_distr: Callable = torch.randn,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"Generates synthetic demonstrations.\n",
        "\n",
        "    Args:\n",
        "        tensor_size (int): Size of the tensor.\n",
        "        n_data (int): Number of demonstrations.\n",
        "        limit_rank (int): Limit rank of each tensor.\n",
        "        prob_distr (Callable, optional): Distribution of the entries of the\n",
        "        tensor.\n",
        "        random_seed (int, optional): Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        torch.random.manual_seed(random_seed)\n",
        "    for _ in range(n_data):\n",
        "        # rank = torch.randint(low=1, high=limit_rank + 1, size=(1,)).item()\n",
        "        rank = limit_rank\n",
        "        output_tensor = torch.zeros(tensor_size, tensor_size, tensor_size)\n",
        "        list_of_triplets = []\n",
        "        for i in range(rank):\n",
        "            valid_triplet = False\n",
        "            while not valid_triplet:\n",
        "                u = prob_distr(tensor_size)\n",
        "                v = prob_distr(tensor_size)\n",
        "                w = prob_distr(tensor_size)\n",
        "                generated_tensor = (\n",
        "                    u.reshape(-1, 1, 1)\n",
        "                    * v.reshape(1, -1, 1)\n",
        "                    * w.reshape(1, 1, -1)\n",
        "                )\n",
        "                if not (generated_tensor == 0).all():\n",
        "                    valid_triplet = True\n",
        "                    list_of_triplets.append((u, v, w))\n",
        "                    output_tensor += generated_tensor\n",
        "        yield output_tensor, list_of_triplets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfY95G21FBKO"
      },
      "source": [
        "# Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "h4hHC9HgK9U7"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "EO2jXC4pqElu"
      },
      "outputs": [],
      "source": [
        "def compute_move(triplets: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
        "    \"\"\"Computes the outer product of the three tensors in the triplet that\n",
        "    will be subtracted from the current state.\n",
        "\n",
        "    Args:\n",
        "        triplets (Tuple[torch.Tensor, torch.Tensor, torch.Tensor]): Tensors u,\n",
        "        v, and w.\n",
        "    \"\"\"\n",
        "    u, v, w = triplets\n",
        "    return u.reshape(-1, 1, 1) * v.reshape(1, -1, 1) * w.reshape(1, 1, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5jWA3-TUFC2P"
      },
      "outputs": [],
      "source": [
        "class SyntheticDataBuffer(Dataset):\n",
        "    \"\"\"Dataset of synthetically generated demonstrations.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tensor_size,\n",
        "        n_data,\n",
        "        limit_rank,\n",
        "        prob_distr,\n",
        "        n_prev_actions: int,\n",
        "        device: str,\n",
        "        n_steps: int,\n",
        "        random_seed=None,\n",
        "    ):\n",
        "        \"\"\"Builds a dataset of synthetic demonstrations.\n",
        "\n",
        "        Args:\n",
        "            tensor_size (int): Size of the tensor.\n",
        "            n_data (int): Number of demonstrations to generate.\n",
        "            limit_rank (int): Maximum rank of the generated tensors.\n",
        "            prob_distr (Callable): Probability distribution to use to generate\n",
        "            the tensors.\n",
        "            n_prev_actions (int): Number of previous actions to use as input.\n",
        "            device (str): Name of the torch device to use.\n",
        "            n_steps (int): Number of steps to perform in the environment.\n",
        "            random_seed (int, optional): Random seed to use.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.len_data = 0\n",
        "        self.n_prev_actions = n_prev_actions\n",
        "        self.limit_rank = limit_rank\n",
        "        self.n_steps = n_steps\n",
        "        self.save_dir = os.path.join(SAVE_DIR_SYNT, f\"size_{tensor_size}\")\n",
        "        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n",
        "        number_of_triplets = len(list(Path(self.save_dir).glob(\"*.pt\"))) // 2\n",
        "        if number_of_triplets < n_data:\n",
        "            self.len_data = number_of_triplets\n",
        "            for i, (output_tensor, list_of_triplets) in enumerate(\n",
        "                generate_synthetic_data(\n",
        "                    tensor_size,\n",
        "                    n_data - number_of_triplets,\n",
        "                    limit_rank,\n",
        "                    prob_distr,\n",
        "                    random_seed,\n",
        "                )\n",
        "            ):\n",
        "                torch.save(\n",
        "                    output_tensor,\n",
        "                    os.path.join(\n",
        "                        self.save_dir, f\"output_tensor_{self.len_data}.pt\"\n",
        "                    ),\n",
        "                )\n",
        "                torch.save(\n",
        "                    list_of_triplets,\n",
        "                    os.path.join(\n",
        "                        self.save_dir, f\"list_of_triplets_{self.len_data}.pt\"\n",
        "                    ),\n",
        "                )\n",
        "                self.len_data += 1\n",
        "        else:\n",
        "            self.len_data = n_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_data * self.limit_rank\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __getitem__(self, idx):\n",
        "        i = idx // self.limit_rank\n",
        "        j = idx % self.limit_rank\n",
        "        output_tensor = torch.load(\n",
        "            os.path.join(self.save_dir, f\"output_tensor_{i}.pt\")\n",
        "        )\n",
        "        list_of_triplets = torch.load(\n",
        "            os.path.join(self.save_dir, f\"list_of_triplets_{i}.pt\")\n",
        "        )\n",
        "        if j != self.limit_rank - 1:\n",
        "            moves = list_of_triplets[j + 1 :]  # noqa E203\n",
        "            output_tensor = self._apply_moves(output_tensor, moves)\n",
        "        triplet = list_of_triplets[j]\n",
        "        output_tensor = torch.stack(\n",
        "            [\n",
        "                output_tensor,\n",
        "                *(\n",
        "                    compute_move(t)\n",
        "                    for t in reversed(\n",
        "                        list_of_triplets[\n",
        "                            j + 1 : j + 1 + self.n_prev_actions  # noqa E203\n",
        "                        ]\n",
        "                    )\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        if len(output_tensor) < self.n_prev_actions + 1:\n",
        "            output_tensor = torch.cat(\n",
        "                [\n",
        "                    output_tensor,\n",
        "                    torch.zeros(\n",
        "                        self.n_prev_actions + 1 - len(output_tensor),\n",
        "                        *output_tensor.shape[1:],\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "        policy = map_triplet_to_action(triplet, base=5, n_steps=self.n_steps)\n",
        "        reward = torch.tensor([-(j + 1)])\n",
        "        scalar = get_scalars(output_tensor, self.limit_rank - j, with_bs=False)\n",
        "        return (\n",
        "            output_tensor.to(self.device),\n",
        "            scalar.to(self.device),\n",
        "            policy.to(self.device),\n",
        "            reward.to(self.device),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _apply_moves(\n",
        "        tensor: torch.Tensor,\n",
        "        moves: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n",
        "    ):\n",
        "        \"\"\"Given an initial state and a list of moves, applies the moves to\n",
        "        the state.\n",
        "\n",
        "        Args:\n",
        "            tensor (torch.Tensor): Initial state.\n",
        "            moves (List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]):\n",
        "            List of moves.\n",
        "        \"\"\"\n",
        "        for u, v, w in moves:\n",
        "            tensor = tensor - u.reshape(-1, 1, 1) * v.reshape(\n",
        "                1, -1, 1\n",
        "            ) * w.reshape(1, 1, -1)\n",
        "        return tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "iA9i683JFA3v"
      },
      "outputs": [],
      "source": [
        "class GameDataBuffer(Dataset):\n",
        "    \"\"\"Buffer to store the data from the games played by the MCTS agent.\"\"\"\n",
        "\n",
        "    def __init__(self, device: str, max_buffer_size: int):\n",
        "        \"\"\"Initializes the buffer.\n",
        "\n",
        "        Args:\n",
        "            device (str): Name of the torch device to use.\n",
        "            max_buffer_size (int): Maximum size of the buffer.\n",
        "        \"\"\"\n",
        "        self.num_games = 0\n",
        "        self.temp_dir = tempfile.mkdtemp(\"game_data_buffer\")\n",
        "        self.game_data = {}\n",
        "        self.max_buffer_size = max_buffer_size\n",
        "        self.device = device\n",
        "\n",
        "    def __del__(self):\n",
        "        shutil.rmtree(self.temp_dir)\n",
        "\n",
        "    def add_game(\n",
        "        self,\n",
        "        states: List[torch.Tensor],\n",
        "        policies: List[torch.Tensor],\n",
        "        rewards: List[torch.Tensor],\n",
        "    ):\n",
        "        \"\"\"Adds a played game to the buffer.\n",
        "\n",
        "        Args:\n",
        "            states (List[torch.Tensor]): Observed game states.\n",
        "            policies (List[torch.Tensor]): List of policies.\n",
        "            rewards (List[torch.Tensor]): Observed rewards.\n",
        "        \"\"\"\n",
        "        self.game_data[self.num_games] = len(states)\n",
        "        torch.save(\n",
        "            states, os.path.join(self.temp_dir, f\"states_{self.num_games}.pt\")\n",
        "        )\n",
        "        torch.save(\n",
        "            policies,\n",
        "            os.path.join(self.temp_dir, f\"policies_{self.num_games}.pt\"),\n",
        "        )\n",
        "        torch.save(\n",
        "            rewards,\n",
        "            os.path.join(self.temp_dir, f\"rewards_{self.num_games}.pt\"),\n",
        "        )\n",
        "        self.num_games += 1\n",
        "        if self.num_games >= self.max_buffer_size:\n",
        "            # remove oldest game. Note that this line is not thread safe. Lock\n",
        "            # should be added if multiple threads are used.\n",
        "            self.num_games = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(self.game_data.values())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __getitem__(self, idx):\n",
        "        i = 0\n",
        "        while idx >= self.game_data[i]:\n",
        "            idx -= self.game_data[i]\n",
        "            i += 1\n",
        "        states = torch.load(os.path.join(self.temp_dir, f\"states_{i}.pt\"))\n",
        "        policies = torch.load(os.path.join(self.temp_dir, f\"policies_{i}.pt\"))\n",
        "        rewards = torch.load(os.path.join(self.temp_dir, f\"rewards_{i}.pt\"))\n",
        "        return (\n",
        "            states[idx].to(self.device),\n",
        "            get_scalars(states[idx], idx, with_bs=False).to(self.device),\n",
        "            policies[idx].to(self.device).argmax(dim=-1),\n",
        "            rewards[idx].to(self.device).reshape(1),\n",
        "        )\n",
        "\n",
        "    def save_game_data(self, path: str):\n",
        "        \"\"\"Copy save_dir content in path and save game_data\n",
        "        in json format\n",
        "        \"\"\"\n",
        "        shutil.copytree(self.temp_dir, path, dirs_exist_ok=True)\n",
        "        with open(os.path.join(path, \"game_data.json\"), \"w\") as f:\n",
        "            json.dump(self.game_data, f)\n",
        "\n",
        "    def load_game_data(self, path: str):\n",
        "        \"\"\"Load game_data from json format and copy content\n",
        "        in save_dir\n",
        "        \"\"\"\n",
        "        with open(os.path.join(path, \"game_data.json\"), \"r\") as f:\n",
        "            self.game_data = json.load(f)\n",
        "        shutil.copytree(path, self.temp_dir)\n",
        "        self.num_games = len(self.game_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "RIapoEXWFKs4"
      },
      "outputs": [],
      "source": [
        "class TensorGameDataset(Dataset):\n",
        "    \"\"\"Dataset to be used for training the AlphaTensor algorithm using both\n",
        "    actor generated and synthetic data. A basis change can be applied to both\n",
        "    the data type with a probability specified in the constructor. The\n",
        "    synthetic data and the actor generated one are stored in two data buffers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        len_data,\n",
        "        pct_synth,\n",
        "        tensor_size,\n",
        "        n_synth_data,\n",
        "        limit_rank,\n",
        "        prob_distr,\n",
        "        action_memory_len: int,\n",
        "        device: str,\n",
        "        n_steps: int,\n",
        "        random_seed=None,\n",
        "    ):\n",
        "        self.synthetic_data_buffer = SyntheticDataBuffer(\n",
        "            tensor_size,\n",
        "            n_synth_data,\n",
        "            limit_rank,\n",
        "            prob_distr,\n",
        "            action_memory_len,\n",
        "            n_steps=n_steps,\n",
        "            device=device,\n",
        "            random_seed=random_seed,\n",
        "        )\n",
        "        self.game_data_buffer = GameDataBuffer(\n",
        "            device=device, max_buffer_size=100000\n",
        "        )\n",
        "        self.best_game_data_buffer = GameDataBuffer(\n",
        "            device=device, max_buffer_size=1000\n",
        "        )\n",
        "        self.len_data = len_data\n",
        "        self.pct_synth = pct_synth\n",
        "        self.pct_best_game = 0\n",
        "        self.synth_bool = torch.ones(len_data, dtype=torch.bool)\n",
        "        self.synth_idx = torch.from_numpy(\n",
        "            np.random.choice(\n",
        "                len(self.synthetic_data_buffer), len_data, \\\n",
        "                replace=True\n",
        "                # replace=False\n",
        "\n",
        "            )\n",
        "        )\n",
        "        self.game_idx = None\n",
        "        self.best_game_idx = None\n",
        "        self.action_memory_len = action_memory_len\n",
        "        self.tensor_size = tensor_size\n",
        "        self.device = device\n",
        "\n",
        "    def change_training_split(self, pct_synth, pct_best_game):\n",
        "        self.pct_synth = pct_synth\n",
        "        self.pct_best_game = pct_best_game\n",
        "\n",
        "    def recompute_synthetic_indexes(self):\n",
        "        if len(self.game_data_buffer) > 0:\n",
        "            self.synth_bool = torch.rand(self.len_data) < self.pct_synth\n",
        "            len_synth_data = self.synth_bool.sum().item()\n",
        "            self.synth_idx = torch.from_numpy(\n",
        "                np.random.choice(\n",
        "                    len(self.synthetic_data_buffer),\n",
        "                    len_synth_data,\n",
        "                    # replace=False,\n",
        "                    replace=True\n",
        "                )\n",
        "            )\n",
        "            if len(self.best_game_data_buffer) > 0 and self.pct_best_game > 0:\n",
        "                len_game_data = int(\n",
        "                    (1 - self.pct_synth - self.pct_best_game) * self.len_data\n",
        "                )\n",
        "                replace_game = len_game_data > len(self.game_data_buffer)\n",
        "                len_best_game_data = (\n",
        "                    self.len_data - len_synth_data - len_game_data\n",
        "                )\n",
        "                replace_best_game = len_best_game_data > len(\n",
        "                    self.best_game_data_buffer\n",
        "                )\n",
        "                self.game_idx = torch.from_numpy(\n",
        "                    np.random.choice(\n",
        "                        len(self.game_data_buffer),\n",
        "                        len_game_data,\n",
        "                        replace=replace_game,\n",
        "                    )\n",
        "                )\n",
        "                self.best_game_idx = torch.from_numpy(\n",
        "                    np.random.choice(\n",
        "                        len(self.best_game_data_buffer),\n",
        "                        len_best_game_data,\n",
        "                        replace=replace_best_game,\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                len_game_data = self.len_data - len_synth_data\n",
        "                replace_game = len_game_data > len(self.game_data_buffer)\n",
        "                self.game_idx = torch.from_numpy(\n",
        "                    np.random.choice(\n",
        "                        len(self.game_data_buffer),\n",
        "                        len_game_data,\n",
        "                        replace=replace_game,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.synth_bool[idx]:\n",
        "            return self.synthetic_data_buffer[\n",
        "                self.synth_idx[self.synth_bool[:idx].sum()]\n",
        "            ]\n",
        "        else:\n",
        "            if self.pct_best_game > 0 and self.best_game_idx is not None:\n",
        "                if idx - self.synth_bool[:idx].sum() < len(self.best_game_idx):\n",
        "                    return self.best_game_data_buffer[\n",
        "                        self.best_game_idx[idx - self.synth_bool[:idx].sum()]\n",
        "                    ]\n",
        "                else:\n",
        "                    return self.game_data_buffer[\n",
        "                        self.game_idx[\n",
        "                            idx\n",
        "                            - self.synth_bool[:idx].sum()\n",
        "                            - len(self.best_game_idx)\n",
        "                        ]\n",
        "                    ]\n",
        "            else:\n",
        "                return self.game_data_buffer[\n",
        "                    self.game_idx[idx - self.synth_bool[:idx].sum()]\n",
        "                ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_data\n",
        "\n",
        "    def add_game(\n",
        "        self,\n",
        "        states: List[torch.Tensor],\n",
        "        policies: List[torch.Tensor],\n",
        "        rewards: List[torch.Tensor],\n",
        "    ):\n",
        "        self.game_data_buffer.add_game(states, policies, rewards)\n",
        "\n",
        "    def add_best_game(\n",
        "        self,\n",
        "        states: List[torch.Tensor],\n",
        "        policies: List[torch.Tensor],\n",
        "        rewards: List[torch.Tensor],\n",
        "    ):\n",
        "        self.best_game_data_buffer.add_game(states, policies, rewards)\n",
        "\n",
        "    def save_game_data(self, path):\n",
        "        self.game_data_buffer.save_game_data(os.path.join(path, \"game_data\"))\n",
        "        self.best_game_data_buffer.save_game_data(\n",
        "            os.path.join(path, \"best_game_data\")\n",
        "        )\n",
        "\n",
        "    def load_game_data(self, path):\n",
        "        self.game_data_buffer.load_game_data(os.path.join(path, \"game_data\"))\n",
        "        self.best_game_data_buffer.load_game_data(\n",
        "            os.path.join(path, \"best_game_data\")\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def input_tensor(self) -> torch.Tensor:\n",
        "        max_matrix_size = int(np.sqrt(self.tensor_size))\n",
        "        input_tensor = torch.zeros(\n",
        "            1,\n",
        "            self.action_memory_len + 1,\n",
        "            self.tensor_size,\n",
        "            self.tensor_size,\n",
        "            self.tensor_size,\n",
        "        )\n",
        "        matrix_dims = (\n",
        "            torch.randint(1, max_matrix_size, (3,))\n",
        "            .detach()\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "            .tolist()\n",
        "        )\n",
        "        operation_tensor = self._build_tensor_game_input(\n",
        "            *matrix_dims, action_memory_len=self.action_memory_len\n",
        "        )\n",
        "\n",
        "        input_tensor[\n",
        "            0,\n",
        "            :,\n",
        "            : operation_tensor.shape[1],\n",
        "            : operation_tensor.shape[2],\n",
        "            : operation_tensor.shape[3],\n",
        "        ] = operation_tensor\n",
        "        return input_tensor.to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_tensor_game_input(\n",
        "        dim_1: int, dim_k: int, dim_2: int, action_memory_len: int\n",
        "    ):\n",
        "        \"\"\"Build the input tensor for the game. The input tensor has shape\n",
        "        (action_memory_len+1, matrix_size**2, matrix_size**2, matrix_size**2).\n",
        "        The first slice represent the matrix multiplication tensor which will\n",
        "        be reduced by the TensorGame algorithm. The other slices represent the\n",
        "        action memory.\n",
        "        \"\"\"\n",
        "        input_tensor = torch.zeros(\n",
        "            action_memory_len + 1, dim_1 * dim_k, dim_k * dim_2, dim_1 * dim_2\n",
        "        )\n",
        "        for r in range(dim_1 * dim_2):\n",
        "            for k in range(dim_k):\n",
        "                input_tensor[\n",
        "                    0, (r // dim_2) * dim_k + k, k * dim_2 + r % dim_2, r\n",
        "                ] = 1\n",
        "        return input_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Tru9-F81F-8n"
      },
      "outputs": [],
      "source": [
        "def f_prob_distribution(size):\n",
        "    \"\"\"Samples a tensor of values from a distribution with a peak at 0 and a\n",
        "    tail at -2 and 2.\n",
        "\n",
        "    Args:\n",
        "        size (int): Number of values to sample.\n",
        "    \"\"\"\n",
        "    f_vals = torch.tensor([-2, -1, 0, 1, 2])\n",
        "    f_probs = torch.tensor([0.001, 0.099, 0.8, 0.099, 0.001]).unsqueeze(0)\n",
        "    f_cum_sum = torch.cumsum(f_probs, dim=-1)\n",
        "    unif_prob = torch.rand((size, 1))\n",
        "    tensor_idx = torch.argmax((unif_prob <= f_cum_sum).int(), dim=1)\n",
        "    tensor = f_vals[tensor_idx]\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Nw0cCV4fGw_U"
      },
      "outputs": [],
      "source": [
        "def get_change_basis_matrix(\n",
        "    tensor_size: int,\n",
        "    n_cob: int,\n",
        "    entry_distribution: Callable = torch.randn,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"Generate a list of change of basis matrices.\n",
        "\n",
        "    Args:\n",
        "        tensor_size (int): Size of the tensor.\n",
        "        n_cob (int): Number of change of basis matrices.\n",
        "        entry_distribution (Callable, optional): Distribution of the entries\n",
        "        of the change of basis matrices.\n",
        "        random_seed (int, optional): Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        torch.random.manual_seed(random_seed)\n",
        "    for _ in range(n_cob):\n",
        "        diag_p = 2 * (torch.rand(tensor_size) > 0.5).float() - 1\n",
        "        diag_l = 2 * (torch.rand(tensor_size) > 0.5).float() - 1\n",
        "        random_matrix = entry_distribution((tensor_size, tensor_size))\n",
        "        p_matrix = torch.diag(diag_p)\n",
        "        l_matrix = torch.diag(diag_l)\n",
        "        p_matrix = p_matrix + torch.triu(random_matrix, diagonal=1)\n",
        "        l_matrix = l_matrix + torch.tril(random_matrix, diagonal=-1)\n",
        "        yield torch.matmul(p_matrix, l_matrix)\n",
        "\n",
        "\n",
        "def cob_entry_prob_distribution(size):\n",
        "    full_size = int(np.prod(size))\n",
        "    vals = torch.tensor([-1, 0, 1])\n",
        "    probs = torch.tensor([0.0075, 0.985, 0.0075]).unsqueeze(0)\n",
        "    cum_sum = torch.cumsum(probs, dim=-1)\n",
        "    unif_prob = torch.rand((full_size, 1))\n",
        "    tensor_idx = torch.argmax((unif_prob <= cum_sum).int(), dim=1)\n",
        "    tensor = vals[tensor_idx]\n",
        "    return tensor.reshape(size)\n",
        "\n",
        "\n",
        "class ChangeOfBasis:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tensor_size: int,\n",
        "        n_cob: int,\n",
        "        cob_prob: float,\n",
        "        device: str,\n",
        "        random_seed: int = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor_size (int): Size of the tensor.\n",
        "            n_cob (int): Number of change of basis matrices.\n",
        "            cob_prob (float): Probability of applying a change of basis.\n",
        "            device (str): Name of the torch device to use.\n",
        "            random_seed (int, optional): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.tmp_dir = Path(SAVE_COB_DIR)\n",
        "        self.tmp_dir.mkdir(exist_ok=True, parents=True)\n",
        "        for i, cob_matrix in enumerate(\n",
        "            get_change_basis_matrix(\n",
        "                tensor_size, n_cob, cob_entry_prob_distribution, random_seed\n",
        "            )\n",
        "        ):\n",
        "            torch.save(cob_matrix, f\"{self.tmp_dir}/cob_matrix_{i}.pt\")\n",
        "        self.tensor_size = tensor_size\n",
        "        self.n_cob = n_cob\n",
        "        self.cob_prob = cob_prob\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, tensor: torch.Tensor, return_basis: bool = False):\n",
        "        \"\"\"Apply a change of basis to a tensor.\n",
        "\n",
        "        Args:\n",
        "            tensor (torch.Tensor): Tensor to apply the change of basis to.\n",
        "            return_basis (bool, optional): Whether to return the change of\n",
        "            basis matrix as well.\n",
        "        \"\"\"\n",
        "        cob_prob = torch.rand(1).item()\n",
        "        if cob_prob > self.cob_prob:\n",
        "            return tensor\n",
        "        random_cob = torch.randint(low=0, high=self.n_cob, size=(1,))\n",
        "        cob_matrix = torch.load(\n",
        "            f\"{self.tmp_dir}/cob_matrix_{int(random_cob)}.pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # apply change of basis to each tensor dimension\n",
        "        inner_tensor = tensor[0, 0]\n",
        "        tensor_size = inner_tensor.shape[-1]\n",
        "        original_shape = inner_tensor.shape\n",
        "        cob_matrix = cob_matrix.transpose(0, 1)\n",
        "        inner_tensor = torch.matmul(\n",
        "            inner_tensor.reshape(-1, tensor_size), cob_matrix\n",
        "        ).reshape(original_shape)\n",
        "        inner_tensor = inner_tensor.permute(0, 2, 1)\n",
        "        inner_tensor = torch.matmul(\n",
        "            inner_tensor.reshape(-1, tensor_size), cob_matrix\n",
        "        ).reshape(original_shape)\n",
        "        inner_tensor = inner_tensor.permute(2, 1, 0)\n",
        "        inner_tensor = torch.matmul(\n",
        "            inner_tensor.reshape(-1, tensor_size), cob_matrix\n",
        "        ).reshape(original_shape)\n",
        "        inner_tensor = inner_tensor.permute(2, 0, 1)\n",
        "        tensor[0, 0] = inner_tensor\n",
        "        if return_basis:\n",
        "            return tensor, cob_matrix.transpose(0, 1)\n",
        "        return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xT-lS6yOac4"
      },
      "source": [
        "# MCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "SHjIC8BkOhJk"
      },
      "outputs": [],
      "source": [
        "def extract_present_state(state: torch.Tensor) -> torch.Tensor:\n",
        "    return state[:, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Elh6VaPNOtnv"
      },
      "outputs": [],
      "source": [
        "def to_hash(tensor: torch.Tensor) -> str:\n",
        "    \"\"\"Converts a tensor to a hash string.\n",
        "\n",
        "    Args:\n",
        "        tensor: The tensor to convert.\n",
        "    \"\"\"\n",
        "    hashable_tensor = \"_\".join(\n",
        "        tensor.reshape(-1).long().detach().cpu().numpy().astype(str).tolist()\n",
        "    )\n",
        "    return hashable_tensor\n",
        "\n",
        "def from_hash(hashable_tensor: str, shape: tuple) -> torch.Tensor:\n",
        "    \"\"\"Converts a hash string back to the original tensor.\n",
        "\n",
        "    Args:\n",
        "        hashable_tensor (str): The hash string.\n",
        "        shape (tuple): The shape of the original tensor.\n",
        "    \"\"\"\n",
        "    return torch.tensor([float(x) for x in hashable_tensor.split(\"_\")]).resize(\n",
        "        shape\n",
        "    )\n",
        "\n",
        "def record_action(tree_dict: Dict, state: str, action: str):\n",
        "    \"\"\"Record the action in the tree dictionary.\n",
        "\n",
        "    Args:\n",
        "        tree_dict (Dict): The tree dictionary.\n",
        "        state (str): The state as a hash string.\n",
        "        action (str): The action as a hash string.\n",
        "    \"\"\"\n",
        "    if state in tree_dict:\n",
        "        tree_dict[state].append(action)\n",
        "    else:\n",
        "        tree_dict[state] = [action]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "4DBKK7EkPHYg"
      },
      "outputs": [],
      "source": [
        "def _recompose_possible_states(reduced_memory_states_dict: Dict):\n",
        "    \"\"\"Recompose the possible states from the reduced memory states.\n",
        "\n",
        "    Args:\n",
        "        reduced_memory_states_dict (Dict): The reduced memory states.\n",
        "    \"\"\"\n",
        "    final_states = reduced_memory_states_dict[\"final_states\"]\n",
        "    previous_actions = reduced_memory_states_dict[\"previous_actions\"]\n",
        "    possible_states = [\n",
        "        torch.cat(\n",
        "            [\n",
        "                final_states[i],\n",
        "                previous_actions,\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        for i in range(len(final_states))\n",
        "    ]\n",
        "    return possible_states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "0V1uUlQiPhYN"
      },
      "outputs": [],
      "source": [
        "def select_future_state(\n",
        "    possible_states: List[torch.Tensor],\n",
        "    q_values: torch.Tensor,\n",
        "    N_s_a: torch.Tensor,\n",
        "    repetitions: Dict[int, list],\n",
        "    c_1: float = 1.25,\n",
        "    c_2: float = 19652,\n",
        "    return_idx: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Select the future state maximizing the upper confidence bound.\"\"\"\n",
        "    # q_values (1, K, 1)\n",
        "    pi = torch.tensor(\n",
        "        [\n",
        "            len(repetitions[i])\n",
        "            for i in range(len(possible_states))\n",
        "            if i in repetitions\n",
        "        ]\n",
        "    ).to(q_values.device)\n",
        "    if pi.shape[0] != N_s_a.shape[1]:\n",
        "        print(pi)\n",
        "        print(pi.shape, q_values.shape, N_s_a.shape)\n",
        "        pi = pi[: N_s_a.shape[1]]\n",
        "    ucb = q_values.reshape(-1) + pi * torch.sqrt(\n",
        "        torch.sum(N_s_a) / (1 + N_s_a)\n",
        "    ) * (c_1 + torch.log((torch.sum(N_s_a) + c_2 + 1) / c_2))\n",
        "    if return_idx:\n",
        "        return ucb.argmax()\n",
        "    return possible_states[ucb.argmax()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "mgSXPFprTKby"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates(reducing_tensor: torch.Tensor):\n",
        "    \"\"\"Remove duplicates from a tensor.\n",
        "\n",
        "    Args:\n",
        "        reducing_tensor (torch.Tensor): The tensor to remove duplicates from.\n",
        "    \"\"\"\n",
        "    # reducing tensor has shape (1, N_mc, S, S, S)\n",
        "    n_mc = reducing_tensor.shape[1]\n",
        "    indexes = []\n",
        "    idx_map = {}\n",
        "    for idx in range(n_mc):\n",
        "        if len(indexes) == 0:\n",
        "            indexes.append(idx)\n",
        "            idx_map[idx] = []\n",
        "        else:\n",
        "            idx_tensor = reducing_tensor[:, idx]\n",
        "            for index in indexes:\n",
        "                if (reducing_tensor[:, index] - idx_tensor == 0).all():\n",
        "                    idx_map[index].append(idx)\n",
        "                    break\n",
        "            else:\n",
        "                indexes.append(idx)\n",
        "                idx_map[idx] = []\n",
        "\n",
        "    # idx_map = {i: len(v) for i, v in enumerate(idx_map.values())}\n",
        "    old_idx_to_new_idx_map = {}\n",
        "    for new_idx, (key, values) in enumerate(idx_map.items()):\n",
        "        old_idx_to_new_idx_map[key] = new_idx\n",
        "        for second_idx in values:\n",
        "            old_idx_to_new_idx_map[second_idx] = new_idx\n",
        "    return (\n",
        "        reducing_tensor[:, indexes],\n",
        "        old_idx_to_new_idx_map,\n",
        "        idx_map,\n",
        "        indexes,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "zuaps1sNTGDw"
      },
      "outputs": [],
      "source": [
        "def extract_children_states_from_actions(\n",
        "    state: torch.Tensor,\n",
        "    actions: torch.Tensor,\n",
        "    vec_cardinality: int = 5,\n",
        "):\n",
        "    \"\"\"Extract the children states from the actions.\n",
        "\n",
        "    Args:\n",
        "        state (torch.Tensor): The state of the game.\n",
        "        actions (torch.Tensor): The actions to apply to the state.\n",
        "        vec_cardinality (int, optional): The cardinality of the vectors.\n",
        "    \"\"\"\n",
        "    # state (1, T, S, S, S)\n",
        "    # actions (1, K, N_steps)\n",
        "    # we assume actions to be with N_steps = 1,\n",
        "    #  and N_logits = |F|^(3S/N_steps). Each action is then mapped in a\n",
        "    #  unique way to a triplet (u, v, w) where each vector has size S.\n",
        "    # vector cardinality represents the number of values it can take an entry\n",
        "    #  of u, v or w.\n",
        "    bs, k, n_steps = actions.shape[:3]\n",
        "    len_token = 3 * state.shape[2] // n_steps\n",
        "    actions = map_action_to_triplet(actions, vec_cardinality, len_token)\n",
        "    actions = actions.reshape(bs, k, n_steps * len_token)\n",
        "    vec_dim = state.shape[2]\n",
        "    u = actions[:, :, :vec_dim].reshape(bs, k, vec_dim, 1, 1)\n",
        "    v = actions[:, :, vec_dim : 2 * vec_dim].reshape(\n",
        "        bs, k, 1, vec_dim, 1\n",
        "    )\n",
        "    w = actions[:, :, 2 * vec_dim :].reshape(bs, k, 1, 1, vec_dim)\n",
        "    reducing_tensor = u * v * w\n",
        "    (\n",
        "        reducing_tensor,\n",
        "        old_idx_to_new_idx,\n",
        "        repetition_map,\n",
        "        not_duplicate_indexes,\n",
        "    ) = remove_duplicates(reducing_tensor)\n",
        "    old_state = state[:, 0]\n",
        "    new_state = old_state.unsqueeze(1) - reducing_tensor\n",
        "    rolling_states = torch.roll(state, 1)[:, 2:]\n",
        "    return (\n",
        "        [\n",
        "            torch.cat(\n",
        "                [\n",
        "                    new_state[:, i : i + 1],  # noqa E203\n",
        "                    reducing_tensor[:, i : i + 1],  # noqa E203\n",
        "                    rolling_states,\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "            for i in range(k)\n",
        "        ],\n",
        "        old_idx_to_new_idx,\n",
        "        repetition_map,\n",
        "        not_duplicate_indexes,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "R0efcJW0TiPF"
      },
      "outputs": [],
      "source": [
        "def _reduce_memory_consumption_before_storing(\n",
        "    possible_states: List[torch.Tensor],\n",
        "):\n",
        "    \"\"\"Reduce the memory consumption before storing the states.\n",
        "\n",
        "    Args:\n",
        "        possible_states (List[torch.Tensor]): The possible states.\n",
        "    \"\"\"\n",
        "    final_states = [state[:, 0:2] for state in possible_states]\n",
        "    previous_actions = possible_states[0][:, 2:]\n",
        "    storing_dict = {\n",
        "        \"final_states\": final_states,\n",
        "        \"previous_actions\": previous_actions,\n",
        "    }\n",
        "    return storing_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def game_is_finished(state):\n",
        "    \"\"\"Tells if the game is finished or not.\n",
        "\n",
        "    Args:\n",
        "        state (torch.Tensor): The state of the game.\n",
        "    \"\"\"\n",
        "    # state size (1, S, S, S)\n",
        "    return (state == 0).all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "dL1BCCeHPBUC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def simulate_game(\n",
        "    model,\n",
        "    state: torch.Tensor,\n",
        "    t_time: int,\n",
        "    max_steps: int,\n",
        "    game_tree: Dict,\n",
        "    states_dict: Dict,\n",
        "    horizon: int = 5,\n",
        "):\n",
        "    \"\"\"Simulates a game from a given state.\n",
        "\n",
        "    Args:\n",
        "        model: The model to use for the simulation.\n",
        "        state (torch.Tensor): The initial state.\n",
        "        t_time (int): The current time step.\n",
        "        max_steps (int): The maximum number of steps to simulate.\n",
        "        game_tree (Dict): The game tree.\n",
        "        states_dict (Dict): The states dictionary.\n",
        "        horizon (int): The horizon to use for the simulation.\n",
        "    \"\"\"\n",
        "    idx = t_time\n",
        "    max_steps = min(max_steps, t_time + horizon)\n",
        "    state_hash = to_hash(extract_present_state(state))\n",
        "    trajectory = []\n",
        "    # selection\n",
        "    while state_hash in game_tree:\n",
        "        (\n",
        "            possible_states_dict,\n",
        "            old_idx_to_new_idx,\n",
        "            repetition_map,\n",
        "            N_s_a,\n",
        "            q_values,\n",
        "            actions,\n",
        "        ) = states_dict[state_hash]\n",
        "        possible_states = _recompose_possible_states(possible_states_dict)\n",
        "        state_idx = select_future_state(\n",
        "            possible_states, q_values, N_s_a, repetition_map, return_idx=True\n",
        "        )\n",
        "        trajectory.append((state_hash, state_idx))  # state_hash, action_idx\n",
        "        future_state = extract_present_state(possible_states[state_idx])\n",
        "        state = possible_states[state_idx]\n",
        "        state_hash = to_hash(future_state)\n",
        "        idx += 1\n",
        "\n",
        "    # expansion\n",
        "    if idx <= max_steps:\n",
        "        trajectory.append((state_hash, None))\n",
        "        if not game_is_finished(extract_present_state(state)):\n",
        "            state = state.to(model.device)\n",
        "            scalars = get_scalars(state, idx).to(state.device)\n",
        "            actions, probs, q_values = model(state, scalars)\n",
        "            (\n",
        "                possible_states,\n",
        "                cloned_idx_to_idx,\n",
        "                repetitions,\n",
        "                not_dupl_indexes,\n",
        "            ) = extract_children_states_from_actions(\n",
        "                state,\n",
        "                actions,\n",
        "            )\n",
        "            not_dupl_actions = actions[:, not_dupl_indexes].to(\"cpu\")\n",
        "            not_dupl_q_values = torch.zeros(not_dupl_actions.shape[:-1]).to(\n",
        "                \"cpu\"\n",
        "            )\n",
        "            N_s_a = torch.zeros_like(not_dupl_q_values).to(\"cpu\")\n",
        "            present_state = extract_present_state(state)\n",
        "            states_dict[to_hash(present_state)] = (\n",
        "                _reduce_memory_consumption_before_storing(possible_states),\n",
        "                cloned_idx_to_idx,\n",
        "                repetitions,\n",
        "                N_s_a,\n",
        "                not_dupl_q_values,\n",
        "                not_dupl_actions,\n",
        "            )\n",
        "            game_tree[to_hash(present_state)] = [\n",
        "                to_hash(extract_present_state(fut_state))\n",
        "                for fut_state in possible_states\n",
        "            ]\n",
        "            leaf_q_value = q_values\n",
        "    else:\n",
        "        leaf_q_value = -int(torch.linalg.matrix_rank(state).sum())\n",
        "    # backup\n",
        "    backward_pass(trajectory, states_dict, leaf_q_value=leaf_q_value)\n",
        "\n",
        "\n",
        "def backward_pass(trajectory, states_dict, leaf_q_value: torch.Tensor):\n",
        "    \"\"\"Backward pass of the montecarlo algorithm\"\"\"\n",
        "    reward = 0\n",
        "    for idx, (state, action_idx) in enumerate(reversed(trajectory)):\n",
        "        if action_idx is None:  # leaf node\n",
        "            reward += leaf_q_value\n",
        "        else:\n",
        "            (\n",
        "                _,\n",
        "                old_idx_to_new_idx,\n",
        "                _,\n",
        "                N_s_a,\n",
        "                q_values,\n",
        "                _,\n",
        "            ) = states_dict[state]\n",
        "            if isinstance(reward, torch.Tensor):\n",
        "                reward = reward.to(q_values.device)\n",
        "            action_idx = int(action_idx)\n",
        "            if action_idx in old_idx_to_new_idx:\n",
        "                not_dupl_index = old_idx_to_new_idx[int(action_idx)]\n",
        "            else:\n",
        "                not_dupl_index = action_idx\n",
        "            reward -= 1\n",
        "            q_values[:, not_dupl_index] = (\n",
        "                N_s_a[:, not_dupl_index] * q_values[:, not_dupl_index] + reward\n",
        "            ) / (N_s_a[:, not_dupl_index] + 1)\n",
        "            N_s_a[:, not_dupl_index] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "S2zdk7SwOlbq"
      },
      "outputs": [],
      "source": [
        "def monte_carlo_tree_search(\n",
        "    model: torch.nn.Module,\n",
        "    state: torch.Tensor,\n",
        "    n_sim: int,\n",
        "    t_time,\n",
        "    n_steps: int,\n",
        "    game_tree: Dict,\n",
        "    state_dict: Dict,\n",
        "):\n",
        "    \"\"\"Runs the monte carlo tree search algorithm.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to use for the simulation.\n",
        "        state (torch.Tensor): The initial state.\n",
        "        n_sim (int): The number of simulations to run.\n",
        "        t_time (int): The current time step.\n",
        "        n_steps (int): The maximum number of steps to simulate.\n",
        "        game_tree (Dict): The game tree.\n",
        "        state_dict (Dict): The dictionary containing the states.\n",
        "    \"\"\"\n",
        "    # Note that game tree is not the full tree, but just the one having as root\n",
        "    #  the current node(state).\n",
        "    # should we accept also previous updated trajectories for the current node?\n",
        "    # is it something we should considering when deciding how many simulations\n",
        "    # we should run? (I think yes)\n",
        "    state_hash = to_hash(extract_present_state(state))\n",
        "    if state_hash in state_dict:\n",
        "        with torch.no_grad():\n",
        "            N_s_a = state_dict[state_hash][3]\n",
        "            n_sim -= int(N_s_a.sum())\n",
        "            n_sim = max(n_sim, 0)\n",
        "\n",
        "    for _ in range(n_sim):\n",
        "        simulate_game(model, state, t_time, n_steps, game_tree, state_dict)\n",
        "    # return next state\n",
        "    possible_states_dict, _, repetitions, N_s_a, q_values, _ = state_dict[\n",
        "        state_hash\n",
        "    ]\n",
        "    possible_states = _recompose_possible_states(possible_states_dict)\n",
        "    next_state_idx = select_future_state(\n",
        "        possible_states, q_values, N_s_a, repetitions, return_idx=True\n",
        "    )\n",
        "    next_state = possible_states[next_state_idx]\n",
        "    return next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "vc6rNQAzQN4v"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad() # not sure here\n",
        "def compute_improved_policy(\n",
        "    state_dict: Dict,\n",
        "    states: List[str],\n",
        "    model_n_steps: int,\n",
        "    model_n_logits: int,\n",
        "    N_bar: int,\n",
        "):\n",
        "    \"\"\"Compute the improved policy given the state_dict, the list of states.\n",
        "    The improved policy is computed as (N_s_a(1/tau) / (N_s_a(1/tau)).sum())\n",
        "    where tau is (log(N_s_a.sum()) / log(N_bar))\n",
        "    \"\"\"\n",
        "    policies = torch.zeros(len(states), model_n_steps, model_n_logits)\n",
        "    N_bar = torch.tensor(N_bar)\n",
        "    for idx, state in enumerate(states):\n",
        "        N_s_a = state_dict[state][3]\n",
        "        actions = state_dict[state][5]\n",
        "        if N_s_a.sum() > N_bar:\n",
        "            tau = (torch.log(N_s_a.sum()) / torch.log(N_bar)).item()\n",
        "        else:\n",
        "            tau = 1\n",
        "        N_s_a = N_s_a ** (1 / tau)\n",
        "        improved_policy = N_s_a / N_s_a.sum()\n",
        "        for sample_id in range(actions.shape[1]):\n",
        "            action_ids = actions[0, sample_id]\n",
        "            for step_id, action_id in enumerate(action_ids):\n",
        "                policies[idx, step_id, action_id] += improved_policy[\n",
        "                    0, sample_id\n",
        "                ]\n",
        "    return policies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "JgYqtpo5Ocb0"
      },
      "outputs": [],
      "source": [
        "def actor_prediction(\n",
        "    model: AlphaTensorModel,\n",
        "    input_tensor: torch.Tensor,\n",
        "    maximum_rank: int,\n",
        "    mc_n_sim: int,\n",
        "    N_bar: int,\n",
        "    return_actions: bool = False,\n",
        "):\n",
        "    \"\"\"Runs the monte carlo tree search algorithm to obtain the next states,\n",
        "    policies and rewards.\n",
        "\n",
        "    Args:\n",
        "        model (AlphaTensorModel): The model to use for the simulation.\n",
        "        input_tensor (torch.Tensor): The initial state.\n",
        "        maximum_rank (int): The maximum number of steps to simulate.\n",
        "        mc_n_sim (int): The number of simulations to run.\n",
        "        N_bar (int): The parameter used to compute the improved policy.\n",
        "        return_actions (bool): If True, only actions are returned.\n",
        "    \"\"\"\n",
        "    # input_tensor has shape (1, T, S, S, S)\n",
        "    state = input_tensor\n",
        "    rank = 0\n",
        "    game_tree = {}\n",
        "    state_dict = {}\n",
        "    hash_states = []\n",
        "    states = []\n",
        "    while rank < maximum_rank:\n",
        "        print(f\"current rank is {rank}\")\n",
        "        states.append(state)\n",
        "        hash_states.append(to_hash(extract_present_state(state)))\n",
        "        state = monte_carlo_tree_search(\n",
        "            model,\n",
        "            state,\n",
        "            mc_n_sim,\n",
        "            rank,\n",
        "            maximum_rank,\n",
        "            game_tree,\n",
        "            state_dict,\n",
        "        )\n",
        "        if game_is_finished(extract_present_state(state)):\n",
        "            break\n",
        "        rank += 1\n",
        "    final_state = extract_present_state(state)\n",
        "    policies = compute_improved_policy(\n",
        "        state_dict, hash_states, model.n_steps, model.n_logits, N_bar\n",
        "    )\n",
        "    reward = (\n",
        "        int(torch.linalg.matrix_rank(final_state).sum())\n",
        "        if not game_is_finished(final_state)\n",
        "        else 0\n",
        "    )\n",
        "    rewards = torch.cumsum(\n",
        "        torch.tensor([-1] * (len(policies) - 1) + [reward]), dim=0\n",
        "    )\n",
        "    if return_actions:\n",
        "        actions = [state_dict[hash_state][5] for hash_state in hash_states]\n",
        "        return actions\n",
        "    # policies do not have the batch size, but states still have it\n",
        "    states = [s.squeeze(0) for s in states]\n",
        "    return states, policies, rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "HLgS7rF1_i1b"
      },
      "outputs": [],
      "source": [
        "def swap_data(\n",
        "    states: List[torch.Tensor],\n",
        "    actions: List[torch.Tensor],\n",
        "):\n",
        "    \"\"\"Swaps the last action with a random one and updates the states\n",
        "    accordingly for a single game.\n",
        "\n",
        "    Args:\n",
        "        states (List[torch.Tensor]): All the states for a single game.\n",
        "        actions (List[torch.Tensor]): All the actions through the game.\n",
        "    \"\"\"\n",
        "    last_action = actions[-1]\n",
        "    swap_index = torch.randint(0, len(states) - 1, (1,)).item()\n",
        "    actions[-1] = actions[swap_index]\n",
        "    actions[swap_index] = last_action\n",
        "\n",
        "    actual_state = states[swap_index]\n",
        "    for i in range(swap_index + 1, len(states) + 1):\n",
        "        prev_action = actions[i - 1]\n",
        "        triplet = map_action_to_triplet(prev_action, vector_size=actual_state.shape[-1])\n",
        "        vector_size = actual_state.shape[-1] // 3\n",
        "        bs = actual_state.shape[0]\n",
        "        u = triplet[:, :vector_size].reshape(bs, -1, 1, 1)\n",
        "        v = triplet[:, vector_size : 2 * vector_size].reshape(bs, 1, -1, 1)\n",
        "        w = triplet[:, 2 * vector_size :].reshape(bs, 1, 1, -1)\n",
        "        reduced_state = u * v * w\n",
        "        fut_state = actual_state[:, 0] - reduced_state\n",
        "        new_state = actual_state[:, 1:].roll(1, dims=1)\n",
        "        new_state[:, 0] = reduced_state\n",
        "        actual_state = torch.cat([fut_state, new_state], dim=1)\n",
        "        states[i] = actual_state\n",
        "    return states, actions\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Trainer for the AlphaTensor model. The trainer does not require an\n",
        "    explicit loss since the loss is computed by the model itself. The trainer\n",
        "    is responsible for both the training step and the acting one, storing\n",
        "    acting performance in a buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: AlphaTensorModel,\n",
        "        tensor_size: int,\n",
        "        n_steps: int,\n",
        "        batch_size: int,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        device: str,\n",
        "        len_data: int,\n",
        "        pct_synth: float,\n",
        "        n_synth_data: int,\n",
        "        limit_rank: int,\n",
        "        n_cob: int,\n",
        "        cob_prob: float,\n",
        "        data_augmentation: bool,\n",
        "        loss_params: Tuple[float, float] = None,\n",
        "        random_seed: int = None,\n",
        "        checkpoint_dir: str = None,\n",
        "        checkpoint_data_dir: Path = None,\n",
        "        extra_devices: List[str] = None,\n",
        "    ):\n",
        "        \"\"\"Initializes the trainer.\n",
        "\n",
        "        Args:\n",
        "            model (AlphaTensorModel): The model to train.\n",
        "            tensor_size (int): Flattened size of the matrices to be multiplied.\n",
        "            n_steps (int): Number of steps used to get a single action out of\n",
        "            a triplet.\n",
        "            batch_size (int): Batch size.\n",
        "            optimizer (torch.optim.Optimizer): The optimizer used to train the\n",
        "            model.\n",
        "            device (str): The name of the torch device used for training.\n",
        "            len_data (int): Number of training samples used (both actor\n",
        "            generated and synthetic).\n",
        "            pct_synth (float): Initial percentage of synthetic samples used\n",
        "            for training.\n",
        "            n_synth_data (int): Number of synthetic training samples.\n",
        "            limit_rank (int): Maximum rank for synthetically-generated\n",
        "            matrices.\n",
        "            n_cob (int): Number of change of basis (cob) used for a single\n",
        "            training sample.\n",
        "            cob_prob (float): Probability of applying a change of basis.\n",
        "            data_augmentation (bool): Whether to randomly swap the last\n",
        "            operation of an episode with another operation.\n",
        "            loss_params (Tuple[float, float]): Alpha and Beta parameters used\n",
        "            in the loss function.\n",
        "            random_seed (int): Randomizing seed.\n",
        "            checkpoint_dir (str): Directory used to store model checkpoints.\n",
        "            checkpoint_data_dir (str): Directory used to store games as JSON\n",
        "            files.\n",
        "            extra_devices (List[str]): Extra devices names used for multi-GPU\n",
        "            training.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.dataset = TensorGameDataset(\n",
        "            len_data,\n",
        "            pct_synth,\n",
        "            tensor_size,\n",
        "            n_synth_data,\n",
        "            limit_rank,\n",
        "            f_prob_distribution,\n",
        "            device=device,\n",
        "            n_steps=n_steps,\n",
        "            action_memory_len=(model.tensor_length - 1),\n",
        "            random_seed=random_seed,\n",
        "        )\n",
        "        print(\"Got initial dataset\")\n",
        "        self.batch_size = batch_size\n",
        "        self.max_rank = limit_rank\n",
        "        if loss_params is None:\n",
        "            self.alpha = 1\n",
        "            self.beta = 1\n",
        "        else:\n",
        "            self.alpha, self.beta = loss_params\n",
        "        self.checkpoint_dir = Path(\n",
        "            checkpoint_dir if checkpoint_dir else BASE_CHECKPOINT_DIR\n",
        "        )\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.checkpoint_data_dir = (\n",
        "            checkpoint_data_dir\n",
        "            if checkpoint_data_dir\n",
        "            else Path(BASE_CHECKPOINT_DATA_DIR)\n",
        "        )\n",
        "        self.checkpoint_data_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.change_of_basis = ChangeOfBasis(\n",
        "            tensor_size, n_cob, cob_prob, device, random_seed\n",
        "        )\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.extra_devices = extra_devices\n",
        "        print(\"Trainer inited\")\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Executes a single training step by optimizing the current model\n",
        "        parameters.\"\"\"\n",
        "        self.dataset.recompute_synthetic_indexes()\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        dl = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        print(\"Training AlphaTensor\")\n",
        "        # accumulation_steps = 4\n",
        "        for states, scalars, policies, rewards in tqdm.tqdm(dl):\n",
        "            loss_policy, loss_value = self.model(\n",
        "                states, scalars, policies, rewards\n",
        "            )\n",
        "            loss = self.alpha * loss_policy + self.beta * loss_value\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # if i % accumulation_steps == 0:\n",
        "            #     self.optimizer.step()\n",
        "            # self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Total loss: {total_loss}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act_step(\n",
        "        self,\n",
        "        input_tensor: torch.Tensor,\n",
        "        n_games: int,\n",
        "        mc_n_sim: int,\n",
        "        N_bar: int,\n",
        "    ):\n",
        "        \"\"\"Runs actors in parallel to generate multiple games starting from\n",
        "        the same input tensor.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (torch.Tensor): The input tensor used to generate the\n",
        "            games.\n",
        "            n_games (int): Number of games to generate / actors to be run in\n",
        "            parallel.\n",
        "            mc_n_sim (int): Number of simulations used in the Monte Carlo tree\n",
        "            search.\n",
        "            N_bar (int): N_bar parameter used to compute tau when improving\n",
        "            the policy.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        best_reward = -1e10\n",
        "        best_game = None\n",
        "\n",
        "        for actor_id in range(n_games):\n",
        "            input_tensor_cob = self.change_of_basis(input_tensor).to(\n",
        "                self.device\n",
        "            )\n",
        "            print(f\"Running actor {actor_id} / {n_games}\")\n",
        "            states, policies, rewards = actor_prediction(\n",
        "                self.model,\n",
        "                input_tensor_cob,\n",
        "                self.max_rank,\n",
        "                mc_n_sim,\n",
        "                N_bar,\n",
        "            )\n",
        "            print(f\"Actor {actor_id} finished. Final reward: {rewards[-1]}\")\n",
        "            if rewards[-1] > best_reward:\n",
        "                print(\"New best actor!\")\n",
        "                best_reward = rewards[-1]\n",
        "                best_game = (states, policies, rewards)\n",
        "            self.dataset.add_game(states, policies, rewards)\n",
        "            if self.data_augmentation:\n",
        "                states, policies = swap_data(states, policies)\n",
        "                self.dataset.add_game(states, policies, rewards)\n",
        "        if best_game is not None:\n",
        "            self.dataset.add_best_game(*best_game)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epochs: int,\n",
        "        n_games: int,\n",
        "        mc_n_sim: int,\n",
        "        N_bar: int,\n",
        "        initial_lr: float,\n",
        "        lr_decay_factor: float,\n",
        "        lr_decay_steps: int,\n",
        "        starting_epoch: int = 0,\n",
        "    ):\n",
        "        \"\"\"Trains the model for a given number of epochs.\n",
        "\n",
        "        Args:\n",
        "            n_epochs (int): Number of training epochs.\n",
        "            n_games (int): Number of games to generate / actors to be run in\n",
        "            parallel at each step.\n",
        "            mc_n_sim (int): Number of simulations used in the Monte Carlo tree\n",
        "            search at each step.\n",
        "            N_bar (int): N_bar parameter used to compute tau when improving\n",
        "            the policy.\n",
        "            initial_lr (float): Initial learning rate.\n",
        "            lr_decay_factor (float): Learning rate's decay factor.\n",
        "            lr_decay_steps (int): Number of learning rate's decay steps.\n",
        "            starting_epoch (int, optional): Epoch from which to start / resume\n",
        "            training.\n",
        "        \"\"\"\n",
        "        self.model = self.model.to(self.device)\n",
        "        if starting_epoch + 1 > n_epochs // 50:\n",
        "            self.dataset.change_training_split(0.7, 0.05)\n",
        "        if (\n",
        "            starting_epoch + 1 > n_epochs // 10\n",
        "        ):  # when restarting from a checkpoint\n",
        "            mc_n_sim = mc_n_sim * 4\n",
        "        for epoch in range(starting_epoch, n_epochs):\n",
        "            if epoch + 1 == n_epochs // 50:\n",
        "                self.dataset.change_training_split(0.7, 0.05)\n",
        "            if epoch + 1 == n_epochs // 10:\n",
        "                mc_n_sim = mc_n_sim * 4\n",
        "            # apply learning rate decay each epoch if epoch < lr_decay_steps\n",
        "            if 0 < epoch < lr_decay_steps - 1:\n",
        "                lr = initial_lr * lr_decay_factor ** (epoch / lr_decay_steps)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group[\"lr\"] = lr\n",
        "\n",
        "            print(f\"Epoch {epoch} / {n_epochs}\")\n",
        "            self.train_step()\n",
        "            if epoch % 10 == 0:\n",
        "                self.act_step(\n",
        "                    self.dataset.input_tensor, n_games, mc_n_sim, N_bar\n",
        "                )\n",
        "            # save checkpoint\n",
        "            if (epoch + 1) % 50 == 0:\n",
        "                checkpoint_name = f\"checkpoint_{epoch + 1}.pt\"\n",
        "                checkpoint = {\n",
        "                    \"model_state_dict\": self.model.state_dict(),\n",
        "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                }\n",
        "                torch.save(\n",
        "                    checkpoint,\n",
        "                    self.checkpoint_dir / checkpoint_name,\n",
        "                )\n",
        "                print(f\"Saving {checkpoint_name} in {self.checkpoint_dir}/\")\n",
        "                self.dataset.save_game_data(self.checkpoint_data_dir)\n",
        "        print(\"Training finished\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Ca9NwcXgStnA"
      },
      "outputs": [],
      "source": [
        "class LoadCheckpointDataOp(): # Operation\n",
        "    \"\"\"An operation which loads the games played while training an\n",
        "    OpenAlphaTensor model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._loaded = False\n",
        "\n",
        "    def execute(self, games_store_dir: Path, trainer: Trainer):\n",
        "        \"\"\"Load the games played while training an OpenAlphaTensor model.\n",
        "\n",
        "        Args:\n",
        "            games_store_dir: The directory where the games are stored.\n",
        "            trainer: The trainer to load the games into.\n",
        "        \"\"\"\n",
        "        games_store_dir = games_store_dir or BASE_CHECKPOINT_DATA_DIR\n",
        "        print(f\"loading checkpoint {games_store_dir}\")\n",
        "        # if games_store_dir contains games, load them\n",
        "        if (\n",
        "            games_store_dir.exists()\n",
        "            and (games_store_dir / \"game_data.json\").exists()\n",
        "        ):\n",
        "            trainer.dataset.load_games(games_store_dir)\n",
        "        self._loaded = True\n",
        "\n",
        "    def get_result(self) -> bool:\n",
        "        \"\"\"Returns whether the games were loaded or not.\"\"\"\n",
        "        return self._loaded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OqU1K_V6DSre"
      },
      "outputs": [],
      "source": [
        "class TrainingOperation(): # Operation\n",
        "    \"\"\"Operation which trains an AlphaTensor model to learn more efficient\n",
        "    matrix multiplications.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._trained_model = None\n",
        "\n",
        "        self._load_checkpoint_data_op = LoadCheckpointDataOp()\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        model: AlphaTensorModel,\n",
        "        input_size: int,\n",
        "        n_steps: int,\n",
        "        batch_size: int,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        device: str,\n",
        "        len_data: int,\n",
        "        pct_synth: float,\n",
        "        n_synth_data: int,\n",
        "        limit_rank: int,\n",
        "        max_epochs: int,\n",
        "        n_actors: int,\n",
        "        mc_n_sim: int,\n",
        "        N_bar: int,\n",
        "        last_epoch: int,\n",
        "        lr: float,\n",
        "        lr_decay_factor: float,\n",
        "        lr_decay_steps: int,\n",
        "        loss_params: Tuple[float, float] = None,\n",
        "        random_seed: int = None,\n",
        "        checkpoint_dir: str = None,\n",
        "        checkpoint_data_dir: str = None,\n",
        "        n_cob: int = 0,\n",
        "        cob_prob: float = 0.0,\n",
        "        data_augmentation: bool = False,\n",
        "        extra_devices: List[str] = None,\n",
        "    ):\n",
        "        \"\"\"Trains an AlphaTensor model to learn more efficient matrix\n",
        "        multiplications.\n",
        "\n",
        "        Args:\n",
        "            model (AlphaTensorModel): The model to be trained.\n",
        "            input_size (int): Flattened size of the matrices to be multiplied.\n",
        "            n_steps (int): Number of steps used to get a single action out of\n",
        "            a triplet.\n",
        "            batch_size (int): Batch size.\n",
        "            optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "            device (str): The name of the torch device used for training.\n",
        "            len_data (int): Number of training samples used (both actor\n",
        "            generated and synthetic).\n",
        "            pct_synth (float): Initial percentage of synthetic samples used\n",
        "            for training.\n",
        "            n_synth_data (int): Number of synthetic training samples.\n",
        "            limit_rank (int): Maximum rank for synthetically-generated\n",
        "            matrices.\n",
        "            max_epochs (int): Number of training epochs.\n",
        "            n_actors (int): Number of actors to play a single each game at\n",
        "            each training step.\n",
        "            mc_n_sim (int): Number of simulations during Monte Carlo tree\n",
        "            search.\n",
        "            N_bar (int): N_bar parameter used to compute tau when improving\n",
        "            the policy.\n",
        "            last_epoch (int): Latest epoch reached during training from which\n",
        "            checkpoint data will be loaded.\n",
        "            lr (float): Learning rate.\n",
        "            lr_decay_factor (float): Learning rate's decay factor.\n",
        "            lr_decay_steps (int): Number of learning rate's decay steps.\n",
        "            loss_params (Tuple[float, float]): Alpha and Beta parameters used\n",
        "            in the loss function.\n",
        "            random_seed (int): Randomizing seed.\n",
        "            checkpoint_dir (str): Directory used to store model checkpoints.\n",
        "            checkpoint_data_dir (str): Directory used to store games as JSON\n",
        "            files.\n",
        "            n_cob (int): Number of change of basis (cob) used for a single\n",
        "            training sample.\n",
        "            cob_prob (float): Probability of applying a change of basis.\n",
        "            data_augmentation (bool): Whether to randomly swap the last\n",
        "            operation of an episode with another operation.\n",
        "            extra_devices (List[str]): Extra devices names used for multi-GPU\n",
        "            training.\n",
        "        \"\"\"\n",
        "        checkpoint_data_dir = Path(checkpoint_data_dir or \"games\")\n",
        "        print(\"Building trainer\")\n",
        "\n",
        "        # build trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            tensor_size=input_size,\n",
        "            n_steps=n_steps,\n",
        "            batch_size=batch_size,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "            len_data=len_data,\n",
        "            pct_synth=pct_synth,\n",
        "            n_synth_data=n_synth_data,\n",
        "            limit_rank=limit_rank,\n",
        "            loss_params=loss_params,\n",
        "            random_seed=random_seed,\n",
        "            checkpoint_dir=checkpoint_dir,\n",
        "            checkpoint_data_dir=checkpoint_data_dir,\n",
        "            data_augmentation=data_augmentation,\n",
        "            cob_prob=cob_prob,\n",
        "            n_cob=n_cob,\n",
        "            extra_devices=extra_devices,\n",
        "        )\n",
        "\n",
        "        # load checkpoint data\n",
        "        self._load_checkpoint_data_op.execute(\n",
        "            games_store_dir=checkpoint_data_dir,\n",
        "            trainer=trainer,\n",
        "        )\n",
        "        print(\"Start training\")\n",
        "        # train\n",
        "        trainer.train(\n",
        "            n_epochs=max_epochs,\n",
        "            n_games=n_actors,\n",
        "            mc_n_sim=mc_n_sim,\n",
        "            N_bar=N_bar,\n",
        "            starting_epoch=last_epoch,\n",
        "            initial_lr=lr,\n",
        "            lr_decay_factor=lr_decay_factor,\n",
        "            lr_decay_steps=lr_decay_steps,\n",
        "        )\n",
        "        self._trained_model = trainer.model\n",
        "\n",
        "    def get_trained_model(self):\n",
        "        \"\"\"Returns the trained model.\"\"\"\n",
        "        return self._trained_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "yQvng8KRT7ge"
      },
      "outputs": [],
      "source": [
        "class BuildModelOp():\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = None\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        tensor_length: int,\n",
        "        input_size: int,\n",
        "        scalars_size: int,\n",
        "        emb_dim: int,\n",
        "        n_steps: int,\n",
        "        n_logits: int,\n",
        "        n_samples: int,\n",
        "    ):\n",
        "        \"\"\"Builds the OpenAlphaTensor model.\n",
        "\n",
        "        Args:\n",
        "            tensor_length (int): Number of tensors to as history.\n",
        "            input_size (int): Flattened size of the matrices to be multiplied.\n",
        "            scalars_size (int): Size of the scalar vectors fed to the torso\n",
        "            model.\n",
        "            emb_dim (int): Embedding dimension.\n",
        "            n_steps (int): Number of steps used to get a single action out of\n",
        "            a triplet.\n",
        "            n_logits (int): Number of logits output by the policy head.\n",
        "            n_samples (int): Number of samples used by the policy head at\n",
        "            evaluation time.\n",
        "        \"\"\"\n",
        "        self._model = AlphaTensorModel(\n",
        "            tensor_length=tensor_length,\n",
        "            input_size=input_size,\n",
        "            scalars_size=scalars_size,\n",
        "            emb_dim=emb_dim,\n",
        "            n_steps=n_steps,\n",
        "            n_logits=n_logits,\n",
        "            n_samples=n_samples,\n",
        "        )\n",
        "\n",
        "    def get_model(self) -> AlphaTensorModel:\n",
        "        return self._model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "1kH-YZlGmljb"
      },
      "outputs": [],
      "source": [
        "class SaveModelOp():\n",
        "    \"\"\"An operation which saves an OpenAlphaTensor model.\n",
        "    The model parameters are stored in a json file, while the model weights\n",
        "    are stored in a .pt file.\"\"\"\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        model: AlphaTensorModel,\n",
        "        save_dir: str,\n",
        "    ):\n",
        "        \"\"\"Saves the OpenAlphaTensor model.\n",
        "\n",
        "        Args:\n",
        "            model (AlphaTensorModel): OpenAlphaTensor model to be saved.\n",
        "            save_dir (str): Directory where the model will be saved.\n",
        "        \"\"\"\n",
        "        save_dir = Path(save_dir if save_dir else \".\")\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        torch.save(model.state_dict(), save_dir / \"final_model.pt\")\n",
        "        model_params = {\n",
        "            \"input_size\": model.input_size,\n",
        "            \"tensor_length\": model.tensor_length,\n",
        "            \"scalars_size\": 1,\n",
        "            \"emb_dim\": model.emb_dim,\n",
        "            \"n_steps\": model.n_steps,\n",
        "            \"n_logits\": model.n_logits,\n",
        "            \"n_samples\": model.n_samples,\n",
        "        }\n",
        "        # save parameters in a json file\n",
        "        with open(save_dir / \"model_params.json\", \"w\") as f:\n",
        "            json.dump(model_params, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "L_xhb5Q7mize"
      },
      "outputs": [],
      "source": [
        "class BuildOptimizerOp():\n",
        "    \"\"\"An operation which builds an optimizer for an OpenAlphaTensor model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._optimizer = None\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        optimizer_name: str,\n",
        "        model: AlphaTensorModel,\n",
        "        lr: float,\n",
        "        weight_decay: float,\n",
        "    ):\n",
        "        \"\"\"Builds the optimizer for the OpenAlphaTensor model.\n",
        "\n",
        "        Args:\n",
        "            optimizer_name (str): Name of the optimizer used.\n",
        "            model (AlphaTensorModel): OpenAlphaTensor model to be trained.\n",
        "            lr (float): Learning rate.\n",
        "            weight_decay (float): Weight decay used by the optimizer.\n",
        "        \"\"\"\n",
        "        if optimizer_name == \"adam\":\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"adamw\":\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                model.parameters(), lr=lr, weight_decay=weight_decay\n",
        "            )\n",
        "        elif optimizer_name == \"sgd\":\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n",
        "        self._optimizer = optimizer\n",
        "\n",
        "    def get_optimizer(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"Returns the built optimizer.\"\"\"\n",
        "        return self._optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "OYI5j--ymwnZ"
      },
      "outputs": [],
      "source": [
        "def optimizer_to(optim: torch.optim.Optimizer, device: str):\n",
        "    for param in optim.state.values():\n",
        "        # Not sure there are any global tensors in the state dict\n",
        "        if isinstance(param, torch.Tensor):\n",
        "            param.data = param.data.to(device)\n",
        "            if param._grad is not None:\n",
        "                param._grad.data = param._grad.data.to(device)\n",
        "        elif isinstance(param, dict):\n",
        "            for subparam in param.values():\n",
        "                if isinstance(subparam, torch.Tensor):\n",
        "                    subparam.data = subparam.data.to(device)\n",
        "                    if subparam._grad is not None:\n",
        "                        subparam._grad.data = subparam._grad.data.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "SrvRoGWvmra0"
      },
      "outputs": [],
      "source": [
        "class LoadCheckPointOp():\n",
        "    \"\"\"An operation which loads a checkpoint during training of an\n",
        "    OpenAlphaTensor model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._last_epoch = None\n",
        "        self._model = None\n",
        "        self._optimizer = None\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        model: AlphaTensorModel,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        checkpoint_dir: str,\n",
        "    ):\n",
        "        \"\"\"Load a checkpoint from a directory.\n",
        "\n",
        "        Args:\n",
        "            model: The model to load the checkpoint into.\n",
        "            optimizer: The optimizer to load the checkpoint into.\n",
        "            checkpoint_dir: The directory to load the checkpoint from.\n",
        "        \"\"\"\n",
        "        checkpoint_dir = checkpoint_dir or BASE_CHECKPOINT_DIR\n",
        "        if (\n",
        "            Path(checkpoint_dir).exists()\n",
        "            and len(list(Path(checkpoint_dir).glob(\"*.pt\"))) > 0\n",
        "        ):\n",
        "\n",
        "            def key_func(x):\n",
        "                return int(x.stem.split(\"_\")[-1])\n",
        "\n",
        "            checkpoint_path = sorted(\n",
        "                Path(checkpoint_dir).glob(\"*.pt\"), key=key_func\n",
        "            )[-1]\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            old_device = model.device\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "            model.to(old_device)\n",
        "            print(f\"Loaded model to {old_device}\")\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "            optimizer_to(optimizer, old_device)\n",
        "            last_epoch = int(checkpoint_path.stem.split(\"_\")[-1])\n",
        "        else:\n",
        "            last_epoch = 0\n",
        "\n",
        "        self._last_epoch = last_epoch\n",
        "        self._model = model\n",
        "        self._optimizer = optimizer\n",
        "\n",
        "    def get_last_epoch(self) -> int:\n",
        "        \"\"\"Returns the last epoch of the loaded checkpoint.\"\"\"\n",
        "        return self._last_epoch\n",
        "\n",
        "    def get_model(self) -> AlphaTensorModel:\n",
        "        \"\"\"Returns the model loaded from the checkpoint.\"\"\"\n",
        "        return self._model\n",
        "\n",
        "    def get_optimizer(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"Returns the optimizer loaded from the checkpoint.\"\"\"\n",
        "        return self._optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "vOcyEoPtS48x"
      },
      "outputs": [],
      "source": [
        "class TrainAlphaTensorRootOp():\n",
        "    \"\"\"Root operation which trains an AlphaTensor model to learn more\n",
        "    efficient matrix multiplications.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = None\n",
        "        self._optimizer = None\n",
        "\n",
        "        self._build_model_op = BuildModelOp()\n",
        "        self._build_optimizer_op = BuildOptimizerOp()\n",
        "        self._load_checkpoint_op = LoadCheckPointOp()\n",
        "        self._training_op = TrainingOperation()\n",
        "        self._save_model_op = SaveModelOp()\n",
        "\n",
        "    def execute(\n",
        "        self,\n",
        "        tensor_length: int,\n",
        "        input_size: int,\n",
        "        scalars_size: int,\n",
        "        emb_dim: int,\n",
        "        n_steps: int,\n",
        "        n_logits: int,\n",
        "        n_samples: int,\n",
        "        optimizer_name: str,\n",
        "        lr: float,\n",
        "        lr_decay_factor: float,\n",
        "        lr_decay_steps: int,\n",
        "        weight_decay: float,\n",
        "        loss_params: Tuple[float, float],\n",
        "        checkpoint_dir: str,\n",
        "        checkpoint_data_dir: str,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        len_data: int,\n",
        "        n_synth_data: int,\n",
        "        pct_synth: float,\n",
        "        limit_rank: int,\n",
        "        n_actors: int,\n",
        "        mc_n_sim: int,\n",
        "        N_bar: int,\n",
        "        device: str,\n",
        "        save_dir: str,\n",
        "        random_seed: int,\n",
        "        n_cob: int,\n",
        "        cob_prob: float,\n",
        "        data_augmentation: bool,\n",
        "        extra_devices: List[str],\n",
        "    ):\n",
        "        \"\"\"Trains an AlphaTensor model to learn more efficient matrix\n",
        "        multiplications.\n",
        "\n",
        "        Args:\n",
        "            tensor_length (int): Number of step tensors fed to the model\n",
        "            (history and current state),\n",
        "            input_size (int): Flattened size of the matrices to be multiplied,\n",
        "            scalars_size (int): Size of the scalar vectors fed to the torso\n",
        "            model,\n",
        "            emb_dim (int): Embedding dimension,\n",
        "            n_steps (int): Number of steps used to get a single action out of\n",
        "            a triplet,\n",
        "            n_logits (int): Number of logits output by the policy head,\n",
        "            n_samples (int): Number of samples used by the policy head at\n",
        "            evaluation time,\n",
        "            optimizer_name (str): Name of the optimizer used,\n",
        "            lr (float): Learning rate,\n",
        "            lr_decay_factor (float): Learning rate's decay factor,\n",
        "            lr_decay_steps (int): Number of learning rate's decay steps,\n",
        "            weight_decay (float): Weight decay used by the optimizer,\n",
        "            loss_params (Tuple[float, float]): Alpha and Beta parameters used\n",
        "            in the loss function,\n",
        "            checkpoint_dir (str): Directory used to store model checkpoints,\n",
        "            checkpoint_data_dir (str): Directory used to store games as JSON\n",
        "            files,\n",
        "            epochs (int): Number of training epochs,\n",
        "            batch_size (int): Batch size,\n",
        "            len_data (int): Number of training samples used (both actor\n",
        "            generated and synthetic),\n",
        "            n_synth_data (int): Number of synthetic training samples,\n",
        "            pct_synth (float): Initial percentage of synthetic samples used\n",
        "            for training,\n",
        "            limit_rank (int): Maximum rank for synthetically-generated\n",
        "            matrices,\n",
        "            n_actors (int): Number of actors to play a single each game at\n",
        "            each training step,\n",
        "            mc_n_sim (int): Number of simulations during Monte Carlo tree\n",
        "            search,\n",
        "            N_bar (int): N_bar parameter used to compute tau when improving\n",
        "            the policy,\n",
        "            device (str): The name of the torch device used for training,\n",
        "            save_dir (str): Directory where the final trained model will be\n",
        "            stored,\n",
        "            random_seed (int): Randomizing seed,\n",
        "            n_cob (int): Number of change of basis (cob) used for a single\n",
        "            training sample,\n",
        "            cob_prob (float): Probability of applying a change of basis,\n",
        "            data_augmentation (bool): Whether to randomly swap the last\n",
        "            operation of an episode with another operation,\n",
        "            extra_devices (List[str]): Extra devices names used for multi-GPU\n",
        "            training.\n",
        "        \"\"\"\n",
        "        if self._model is None:\n",
        "            self._build_model_op.execute(\n",
        "                tensor_length=tensor_length,\n",
        "                input_size=input_size,\n",
        "                scalars_size=scalars_size,\n",
        "                emb_dim=emb_dim,\n",
        "                n_steps=n_steps,\n",
        "                n_logits=n_logits,\n",
        "                n_samples=n_samples,\n",
        "            )\n",
        "            self._model = self._build_model_op.get_model().to(device)\n",
        "\n",
        "        if self._build_model_op.get_model() is not None:\n",
        "            self._build_optimizer_op.execute(\n",
        "                optimizer_name=optimizer_name,\n",
        "                model=self._build_model_op.get_model(),\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "            )\n",
        "            self._optimizer = self._build_optimizer_op.get_optimizer()\n",
        "\n",
        "        if self._model is not None and self._optimizer is not None:\n",
        "            self._load_checkpoint_op.execute(\n",
        "                self._model, self._optimizer, checkpoint_dir\n",
        "            )\n",
        "\n",
        "        if self._load_checkpoint_op.get_model() is not None:\n",
        "            self._model = self._load_checkpoint_op.get_model()\n",
        "            self._optimizer = self._load_checkpoint_op.get_optimizer()\n",
        "            starting_epoch = self._load_checkpoint_op.get_last_epoch()\n",
        "            self._training_op.execute(\n",
        "                model=self._model,\n",
        "                input_size=input_size,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                optimizer=self._optimizer,\n",
        "                device=device,\n",
        "                len_data=len_data,\n",
        "                pct_synth=pct_synth,\n",
        "                n_synth_data=n_synth_data,\n",
        "                limit_rank=limit_rank,\n",
        "                max_epochs=epochs,\n",
        "                n_actors=n_actors,\n",
        "                mc_n_sim=mc_n_sim,\n",
        "                N_bar=N_bar,\n",
        "                last_epoch=starting_epoch,\n",
        "                lr=lr,\n",
        "                lr_decay_factor=lr_decay_factor,\n",
        "                lr_decay_steps=lr_decay_steps,\n",
        "                loss_params=loss_params,\n",
        "                random_seed=random_seed,\n",
        "                checkpoint_dir=checkpoint_dir,\n",
        "                checkpoint_data_dir=checkpoint_data_dir,\n",
        "                n_cob=n_cob,\n",
        "                cob_prob=cob_prob,\n",
        "                data_augmentation=data_augmentation,\n",
        "                extra_devices=extra_devices,\n",
        "            )\n",
        "        if self._training_op.get_trained_model() is not None:\n",
        "            self._model = self._training_op.get_trained_model()\n",
        "            self._save_model_op.execute(\n",
        "                model=self._model,\n",
        "                save_dir=save_dir,\n",
        "            )\n",
        "\n",
        "    def get_result(self) -> AlphaTensorModel:\n",
        "        \"\"\"Returns the trained torch model\"\"\"\n",
        "        return self._model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "olp-r3AhnDq7"
      },
      "outputs": [],
      "source": [
        "def train_alpha_tensor(\n",
        "    tensor_length: int,\n",
        "    input_size: int,\n",
        "    scalars_size: int,\n",
        "    emb_dim: int,\n",
        "    n_steps: int,\n",
        "    n_logits: int,\n",
        "    n_samples: int,\n",
        "    optimizer_name: str,\n",
        "    lr: float,\n",
        "    lr_decay_factor: float,\n",
        "    lr_decay_steps: int,\n",
        "    weight_decay: float,\n",
        "    loss_params: Tuple[float, float],\n",
        "    checkpoint_dir: str,\n",
        "    checkpoint_data_dir: str,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    len_data: int,\n",
        "    n_synth_data: int,\n",
        "    pct_synth: float,\n",
        "    limit_rank: int,\n",
        "    n_actors: int,\n",
        "    mc_n_sim: int,\n",
        "    N_bar: int,\n",
        "    device: str,\n",
        "    save_dir: str,\n",
        "    random_seed: int,\n",
        "    n_cob: int,\n",
        "    cob_prob: float,\n",
        "    data_augmentation: bool,\n",
        "    extra_devices: List[str],\n",
        "):\n",
        "    \"\"\"Trains an AlphaTensor model to learn more efficient matrix\n",
        "    multiplications and returns it.\n",
        "\n",
        "    Args:\n",
        "        tensor_length (int): Number of tensors to as history.\n",
        "        input_size (int): Flattened size of the matrices to be multiplied.\n",
        "        scalars_size (int): Size of the scalar vectors fed to the torso model.\n",
        "        emb_dim (int): Embedding dimension.\n",
        "        n_steps (int): Number of steps used to get a single action out of a\n",
        "        triplet.\n",
        "        n_logits (int): Number of logits output by the policy head.\n",
        "        n_samples (int): Number of samples used by the policy head at\n",
        "        evaluation time.\n",
        "        optimizer_name (str): Name of the optimizer used.\n",
        "        lr (float): Learning rate.\n",
        "        lr_decay_factor (float): Learning rate's decay factor.\n",
        "        lr_decay_steps (int): Number of learning rate's decay steps.\n",
        "        weight_decay (float): Weight decay used by the optimizer.\n",
        "        loss_params (Tuple[float, float]): Alpha and Beta parameters used in\n",
        "        the loss function.\n",
        "        checkpoint_dir (str): Directory used to store model checkpoints.\n",
        "        checkpoint_data_dir (str): Directory used to store games as JSON files.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size.\n",
        "        len_data (int): Number of training samples used (both actor generated\n",
        "        and synthetic).\n",
        "        n_synth_data (int): Number of synthetic training samples.\n",
        "        pct_synth (float): Initial percentage of synthetic samples used for\n",
        "        training.\n",
        "        limit_rank (int): Maximum number of steps per episode and maximum rank\n",
        "        for synthetically-generated matrices.\n",
        "        n_actors (int): Number of actors to play a single each game at each\n",
        "        training step.\n",
        "        mc_n_sim (int): Number of simulations during Monte Carlo tree search.\n",
        "        N_bar (int): N_bar parameter used to compute tau when improving the\n",
        "        policy.\n",
        "        device (str): The name of the torch device used for training.\n",
        "        save_dir (str): Directory where the final trained model will be stored.\n",
        "        random_seed (int): Randomizing seed.\n",
        "        n_cob (int): Number of change of basis (cob) used for a single\n",
        "        training sample.\n",
        "        cob_prob (float): Probability of applying a change of basis.\n",
        "        data_augmentation (bool): Whether to randomly swap the last operation\n",
        "        of an episode with another operation.\n",
        "        extra_devices (List[str]): Extra devices names used for multi-GPU\n",
        "        training.\n",
        "    \"\"\"\n",
        "    root_op = TrainAlphaTensorRootOp()\n",
        "    root_op.execute(\n",
        "        tensor_length=tensor_length,\n",
        "        input_size=input_size,\n",
        "        scalars_size=scalars_size,\n",
        "        emb_dim=emb_dim,\n",
        "        n_steps=n_steps,\n",
        "        n_logits=n_logits,\n",
        "        n_samples=n_samples,\n",
        "        optimizer_name=optimizer_name,\n",
        "        lr=lr,\n",
        "        lr_decay_factor=lr_decay_factor,\n",
        "        lr_decay_steps=lr_decay_steps,\n",
        "        weight_decay=weight_decay,\n",
        "        loss_params=loss_params,\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "        checkpoint_data_dir=checkpoint_data_dir,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        len_data=len_data,\n",
        "        n_synth_data=n_synth_data,\n",
        "        pct_synth=pct_synth,\n",
        "        limit_rank=limit_rank,\n",
        "        n_actors=n_actors,\n",
        "        mc_n_sim=mc_n_sim,\n",
        "        N_bar=N_bar,\n",
        "        device=device,\n",
        "        save_dir=save_dir,\n",
        "        random_seed=random_seed,\n",
        "        n_cob=n_cob,\n",
        "        cob_prob=cob_prob,\n",
        "        data_augmentation=data_augmentation,\n",
        "        extra_devices=extra_devices,\n",
        "    )\n",
        "    return root_op.get_result()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "_2Oq9OShNWME"
      },
      "outputs": [],
      "source": [
        "def compute_largest_divisor(n: int) -> int:\n",
        "    for i in range(n // 2, 0, -1):\n",
        "        if n % i == 0:\n",
        "            return i\n",
        "    return 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 8\n",
        "    max_epochs = 6000\n",
        "    action_memory = 7\n",
        "    optimizer = \"adamw\"\n",
        "    weight_decay = 1e-5\n",
        "    lr = 1e-4\n",
        "    lr_decay_factor = 0.1\n",
        "    lr_decay_steps = 5000\n",
        "    device = \"cuda\"\n",
        "    len_data = 2048\n",
        "    pct_synth = 0.9\n",
        "    n_synth_data = 1000\n",
        "    limit_rank = 80\n",
        "    alpha = 1.0\n",
        "    beta = 1.0\n",
        "    random_seed = 42\n",
        "    checkpoint_dir = None\n",
        "    checkpoint_data_dir = None\n",
        "    matrix_size = 4\n",
        "    embed_dim = 1024\n",
        "    actions_sampled = 32\n",
        "    n_actors = 1\n",
        "    mc_n_sim = 2 # more\n",
        "    n_cob = 100\n",
        "    cob_prob = 0.9983  # 1 - 0.0017\n",
        "    data_augmentation = True\n",
        "    N_bar = 100\n",
        "    save_dir = None\n",
        "    cardinality_vector = 5\n",
        "    input_size = matrix_size**2\n",
        "    n_steps = compute_largest_divisor(input_size)\n",
        "    n_actions = cardinality_vector ** (3 * input_size // n_steps)\n",
        "    loss_params = (alpha, beta)\n",
        "\n",
        "    train_alpha_tensor(\n",
        "        tensor_length=action_memory + 1,\n",
        "        input_size=input_size,\n",
        "        scalars_size=1,\n",
        "        emb_dim=embed_dim,\n",
        "        n_steps=n_steps,\n",
        "        n_logits=n_actions,\n",
        "        n_samples=actions_sampled,\n",
        "        device=device,\n",
        "        len_data=len_data,\n",
        "        n_synth_data=n_synth_data,\n",
        "        pct_synth=pct_synth,\n",
        "        batch_size=batch_size,\n",
        "        epochs=max_epochs,\n",
        "        lr=lr,\n",
        "        lr_decay_factor=lr_decay_factor,\n",
        "        lr_decay_steps=lr_decay_steps,\n",
        "        weight_decay=weight_decay,\n",
        "        optimizer_name=optimizer,\n",
        "        loss_params=loss_params,\n",
        "        limit_rank=limit_rank,\n",
        "        random_seed=random_seed,\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "        checkpoint_data_dir=checkpoint_data_dir,\n",
        "        n_actors=n_actors,\n",
        "        mc_n_sim=mc_n_sim,\n",
        "        n_cob=n_cob,\n",
        "        cob_prob=cob_prob,\n",
        "        data_augmentation=data_augmentation or False,\n",
        "        N_bar=N_bar,\n",
        "        extra_devices=[],\n",
        "        save_dir=save_dir,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pn4yRMZZnOO_",
        "outputId": "6089b372-d092-4b08-afca-f63c70365003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build policy head\n",
            "Build value head\n",
            "Building trainer\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# torch.cuda.empty_cache() # put it somewhere\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m main()\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m n_actions \u001b[39m=\u001b[39m cardinality_vector \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m input_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss_params \u001b[39m=\u001b[39m (alpha, beta)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m train_alpha_tensor(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     tensor_length\u001b[39m=\u001b[39;49maction_memory \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     input_size\u001b[39m=\u001b[39;49minput_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     scalars_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     emb_dim\u001b[39m=\u001b[39;49membed_dim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     n_logits\u001b[39m=\u001b[39;49mn_actions,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     n_samples\u001b[39m=\u001b[39;49mactions_sampled,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     len_data\u001b[39m=\u001b[39;49mlen_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     n_synth_data\u001b[39m=\u001b[39;49mn_synth_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     pct_synth\u001b[39m=\u001b[39;49mpct_synth,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     lr_decay_factor\u001b[39m=\u001b[39;49mlr_decay_factor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     lr_decay_steps\u001b[39m=\u001b[39;49mlr_decay_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     optimizer_name\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     loss_params\u001b[39m=\u001b[39;49mloss_params,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     limit_rank\u001b[39m=\u001b[39;49mlimit_rank,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     checkpoint_dir\u001b[39m=\u001b[39;49mcheckpoint_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     checkpoint_data_dir\u001b[39m=\u001b[39;49mcheckpoint_data_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     n_actors\u001b[39m=\u001b[39;49mn_actors,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     mc_n_sim\u001b[39m=\u001b[39;49mmc_n_sim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     n_cob\u001b[39m=\u001b[39;49mn_cob,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     cob_prob\u001b[39m=\u001b[39;49mcob_prob,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     N_bar\u001b[39m=\u001b[39;49mN_bar,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     extra_devices\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m )\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mtrain_alpha_tensor\u001b[1;34m(tensor_length, input_size, scalars_size, emb_dim, n_steps, n_logits, n_samples, optimizer_name, lr, lr_decay_factor, lr_decay_steps, weight_decay, loss_params, checkpoint_dir, checkpoint_data_dir, epochs, batch_size, len_data, n_synth_data, pct_synth, limit_rank, n_actors, mc_n_sim, N_bar, device, save_dir, random_seed, n_cob, cob_prob, data_augmentation, extra_devices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Trains an AlphaTensor model to learn more efficient matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mmultiplications and returns it.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m    training.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m root_op \u001b[39m=\u001b[39m TrainAlphaTensorRootOp()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m root_op\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     tensor_length\u001b[39m=\u001b[39;49mtensor_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     input_size\u001b[39m=\u001b[39;49minput_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     scalars_size\u001b[39m=\u001b[39;49mscalars_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     emb_dim\u001b[39m=\u001b[39;49memb_dim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     n_logits\u001b[39m=\u001b[39;49mn_logits,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     n_samples\u001b[39m=\u001b[39;49mn_samples,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     optimizer_name\u001b[39m=\u001b[39;49moptimizer_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     lr_decay_factor\u001b[39m=\u001b[39;49mlr_decay_factor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     lr_decay_steps\u001b[39m=\u001b[39;49mlr_decay_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     loss_params\u001b[39m=\u001b[39;49mloss_params,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     checkpoint_dir\u001b[39m=\u001b[39;49mcheckpoint_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     checkpoint_data_dir\u001b[39m=\u001b[39;49mcheckpoint_data_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     len_data\u001b[39m=\u001b[39;49mlen_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     n_synth_data\u001b[39m=\u001b[39;49mn_synth_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     pct_synth\u001b[39m=\u001b[39;49mpct_synth,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     limit_rank\u001b[39m=\u001b[39;49mlimit_rank,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     n_actors\u001b[39m=\u001b[39;49mn_actors,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     mc_n_sim\u001b[39m=\u001b[39;49mmc_n_sim,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     N_bar\u001b[39m=\u001b[39;49mN_bar,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     n_cob\u001b[39m=\u001b[39;49mn_cob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     cob_prob\u001b[39m=\u001b[39;49mcob_prob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     extra_devices\u001b[39m=\u001b[39;49mextra_devices,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mreturn\u001b[39;00m root_op\u001b[39m.\u001b[39mget_result()\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mTrainAlphaTensorRootOp.execute\u001b[1;34m(self, tensor_length, input_size, scalars_size, emb_dim, n_steps, n_logits, n_samples, optimizer_name, lr, lr_decay_factor, lr_decay_steps, weight_decay, loss_params, checkpoint_dir, checkpoint_data_dir, epochs, batch_size, len_data, n_synth_data, pct_synth, limit_rank, n_actors, mc_n_sim, N_bar, device, save_dir, random_seed, n_cob, cob_prob, data_augmentation, extra_devices)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_checkpoint_op\u001b[39m.\u001b[39mget_optimizer()\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     starting_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_checkpoint_op\u001b[39m.\u001b[39mget_last_epoch()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_training_op\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m         input_size\u001b[39m=\u001b[39;49minput_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m         len_data\u001b[39m=\u001b[39;49mlen_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m         pct_synth\u001b[39m=\u001b[39;49mpct_synth,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m         n_synth_data\u001b[39m=\u001b[39;49mn_synth_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         limit_rank\u001b[39m=\u001b[39;49mlimit_rank,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         max_epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m         n_actors\u001b[39m=\u001b[39;49mn_actors,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m         mc_n_sim\u001b[39m=\u001b[39;49mmc_n_sim,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m         N_bar\u001b[39m=\u001b[39;49mN_bar,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m         last_epoch\u001b[39m=\u001b[39;49mstarting_epoch,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m         lr_decay_factor\u001b[39m=\u001b[39;49mlr_decay_factor,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m         lr_decay_steps\u001b[39m=\u001b[39;49mlr_decay_steps,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m         loss_params\u001b[39m=\u001b[39;49mloss_params,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m         random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m         checkpoint_dir\u001b[39m=\u001b[39;49mcheckpoint_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m         checkpoint_data_dir\u001b[39m=\u001b[39;49mcheckpoint_data_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m         n_cob\u001b[39m=\u001b[39;49mn_cob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m         cob_prob\u001b[39m=\u001b[39;49mcob_prob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m         data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m         extra_devices\u001b[39m=\u001b[39;49mextra_devices,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_op\u001b[39m.\u001b[39mget_trained_model() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_op\u001b[39m.\u001b[39mget_trained_model()\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mTrainingOperation.execute\u001b[1;34m(self, model, input_size, n_steps, batch_size, optimizer, device, len_data, pct_synth, n_synth_data, limit_rank, max_epochs, n_actors, mc_n_sim, N_bar, last_epoch, lr, lr_decay_factor, lr_decay_steps, loss_params, random_seed, checkpoint_dir, checkpoint_data_dir, n_cob, cob_prob, data_augmentation, extra_devices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBuilding trainer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# build trainer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     tensor_size\u001b[39m=\u001b[39;49minput_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     len_data\u001b[39m=\u001b[39;49mlen_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     pct_synth\u001b[39m=\u001b[39;49mpct_synth,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     n_synth_data\u001b[39m=\u001b[39;49mn_synth_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     limit_rank\u001b[39m=\u001b[39;49mlimit_rank,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     loss_params\u001b[39m=\u001b[39;49mloss_params,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     checkpoint_dir\u001b[39m=\u001b[39;49mcheckpoint_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     checkpoint_data_dir\u001b[39m=\u001b[39;49mcheckpoint_data_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     cob_prob\u001b[39m=\u001b[39;49mcob_prob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     n_cob\u001b[39m=\u001b[39;49mn_cob,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     extra_devices\u001b[39m=\u001b[39;49mextra_devices,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# load checkpoint data\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_checkpoint_data_op\u001b[39m.\u001b[39mexecute(\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     games_store_dir\u001b[39m=\u001b[39mcheckpoint_data_dir,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     trainer\u001b[39m=\u001b[39mtrainer,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m )\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, tensor_size, n_steps, batch_size, optimizer, device, len_data, pct_synth, n_synth_data, limit_rank, n_cob, cob_prob, data_augmentation, loss_params, random_seed, checkpoint_dir, checkpoint_data_dir, extra_devices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m TensorGameDataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     len_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     pct_synth,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     tensor_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     n_synth_data,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     limit_rank,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     f_prob_distribution,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     action_memory_len\u001b[39m=\u001b[39;49m(model\u001b[39m.\u001b[39;49mtensor_length \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGot initial dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mTensorGameDataset.__init__\u001b[1;34m(self, len_data, pct_synth, tensor_size, n_synth_data, limit_rank, prob_distr, action_memory_len, device, n_steps, random_seed)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     len_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     random_seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynthetic_data_buffer \u001b[39m=\u001b[39m SyntheticDataBuffer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         tensor_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         n_synth_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         limit_rank,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         prob_distr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         action_memory_len,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame_data_buffer \u001b[39m=\u001b[39m GameDataBuffer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         device\u001b[39m=\u001b[39mdevice, max_buffer_size\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_game_data_buffer \u001b[39m=\u001b[39m GameDataBuffer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         device\u001b[39m=\u001b[39mdevice, max_buffer_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mSyntheticDataBuffer.__init__\u001b[1;34m(self, tensor_size, n_data, limit_rank, prob_distr, n_prev_actions, device, n_steps, random_seed)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m number_of_triplets \u001b[39m<\u001b[39m n_data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlen_data \u001b[39m=\u001b[39m number_of_triplets\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (output_tensor, list_of_triplets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         generate_synthetic_data(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m             tensor_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             n_data \u001b[39m-\u001b[39m number_of_triplets,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             limit_rank,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             prob_distr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             random_seed,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     ):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m             output_tensor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moutput_tensor_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlen_data\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m             list_of_triplets,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlist_of_triplets_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlen_data\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         )\n",
            "\u001b[1;32mf:\\University\\unn\\GPGPU_SCIENCE\\AlphaTensorMyClone\\MyAlphaTensor.ipynb Cell 56\u001b[0m line \u001b[0;36mgenerate_synthetic_data\u001b[1;34m(tensor_size, n_data, limit_rank, prob_distr, random_seed)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_data):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# rank = torch.randint(low=1, high=limit_rank + 1, size=(1,)).item()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     rank \u001b[39m=\u001b[39m limit_rank\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     output_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(tensor_size, tensor_size, tensor_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     list_of_triplets \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/University/unn/GPGPU_SCIENCE/AlphaTensorMyClone/MyAlphaTensor.ipynb#Y106sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(rank):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# torch.cuda.empty_cache() # put it somewhere\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
